{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.017861,
     "end_time": "2021-03-06T15:35:55.607061",
     "exception": false,
     "start_time": "2021-03-06T15:35:55.589200",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Directory settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-06T15:35:55.647075Z",
     "iopub.status.busy": "2021-03-06T15:35:55.646414Z",
     "iopub.status.idle": "2021-03-06T15:35:55.650284Z",
     "shell.execute_reply": "2021-03-06T15:35:55.649502Z"
    },
    "papermill": {
     "duration": 0.026811,
     "end_time": "2021-03-06T15:35:55.650494",
     "exception": false,
     "start_time": "2021-03-06T15:35:55.623683",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =======================================================\n",
    "# Directory settings\n",
    "# =======================================================\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "OUTPUT_DIR = './'\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "    \n",
    "TRAIN_PATH = '/home/yuki/RANZCR/input/ranzcr-clip-catheter-line-classification/train'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.016423,
     "end_time": "2021-03-06T15:35:55.683834",
     "exception": false,
     "start_time": "2021-03-06T15:35:55.667411",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-06T15:35:55.720681Z",
     "iopub.status.busy": "2021-03-06T15:35:55.719727Z",
     "iopub.status.idle": "2021-03-06T15:35:56.028186Z",
     "shell.execute_reply": "2021-03-06T15:35:56.027641Z"
    },
    "papermill": {
     "duration": 0.328157,
     "end_time": "2021-03-06T15:35:56.028341",
     "exception": false,
     "start_time": "2021-03-06T15:35:55.700184",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('/home/yuki/RANZCR/input/ranzcr-clip-catheter-line-classification/train.csv')\n",
    "test = pd.read_csv('/home/yuki/RANZCR/input/ranzcr-clip-catheter-line-classification/sample_submission.csv')\n",
    "train_annotations = pd.read_csv('/home/yuki/RANZCR/input/ranzcr-clip-catheter-line-classification/train_annotations.csv')\n",
    "\n",
    "# delete suspicious data\n",
    "train = train[train['StudyInstanceUID'] != '1.2.826.0.1.3680043.8.498.93345761486297843389996628528592497280'].reset_index(drop=True)\n",
    "train_annotations = train_annotations[train_annotations['StudyInstanceUID'] != '1.2.826.0.1.3680043.8.498.93345761486297843389996628528592497280'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.016032,
     "end_time": "2021-03-06T15:35:56.060756",
     "exception": false,
     "start_time": "2021-03-06T15:35:56.044724",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## CFG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-06T15:35:56.103470Z",
     "iopub.status.busy": "2021-03-06T15:35:56.102731Z",
     "iopub.status.idle": "2021-03-06T15:35:56.106035Z",
     "shell.execute_reply": "2021-03-06T15:35:56.105540Z"
    },
    "papermill": {
     "duration": 0.029143,
     "end_time": "2021-03-06T15:35:56.106209",
     "exception": false,
     "start_time": "2021-03-06T15:35:56.077066",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# CFG\n",
    "# ====================================================\n",
    "class CFG:\n",
    "    debug=False\n",
    "    device='TPU' # ['TPU', 'GPU']\n",
    "    nprocs=1 # [1, 8]\n",
    "    print_freq=100\n",
    "    num_workers=4\n",
    "    model_name='resnet200d_320'\n",
    "    size=640\n",
    "    scheduler='CosineAnnealingWarmRestarts' # ['ReduceLROnPlateau', 'CosineAnnealingLR', 'CosineAnnealingWarmRestarts']\n",
    "    teacher='/home/yuki/RANZCR/input/005-training-resnext-step1-data/resnet200d_320_fold0_best_loss_cpu.pth'\n",
    "    student='/home/yuki/RANZCR/nb/nb006/ver22/resnet200d_320_fold0_best_loss.pth'\n",
    "    epochs=5\n",
    "    #factor=0.2 # ReduceLROnPlateau\n",
    "    #patience=4 # ReduceLROnPlateau\n",
    "    #eps=1e-6 # ReduceLROnPlateau\n",
    "    T_max=5 # CosineAnnealingLR\n",
    "    T_0=5 # CosineAnnealingWarmRestarts\n",
    "    lr=2e-5 # 1e-4\n",
    "    min_lr=1e-6\n",
    "    batch_size=16 # 64\n",
    "    weight_decay=1e-6\n",
    "    gradient_accumulation_steps=1\n",
    "    max_grad_norm=1000\n",
    "    seed=416\n",
    "    target_size=11\n",
    "    target_cols=['ETT - Abnormal', 'ETT - Borderline', 'ETT - Normal',\n",
    "                 'NGT - Abnormal', 'NGT - Borderline', 'NGT - Incompletely Imaged', 'NGT - Normal', \n",
    "                 'CVC - Abnormal', 'CVC - Borderline', 'CVC - Normal',\n",
    "                 'Swan Ganz Catheter Present']\n",
    "    n_fold=5\n",
    "    trn_fold=[1] # [0, 1, 2, 3, 4]\n",
    "    train=True\n",
    "    \n",
    "if CFG.debug:\n",
    "    CFG.epochs = 3\n",
    "    train = train.sample(n=3000, random_state=CFG.seed).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-06T15:35:56.148825Z",
     "iopub.status.busy": "2021-03-06T15:35:56.148025Z",
     "iopub.status.idle": "2021-03-06T15:37:03.216163Z",
     "shell.execute_reply": "2021-03-06T15:37:03.215254Z"
    },
    "papermill": {
     "duration": 67.093391,
     "end_time": "2021-03-06T15:37:03.216328",
     "exception": false,
     "start_time": "2021-03-06T15:35:56.122937",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if CFG.device == 'TPU':\n",
    "    import os\n",
    "    os.system('curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py')\n",
    "    os.system('python pytorch-xla-env-setup.py --version 1.7 --apt-packages libomp5 libopenblas-dev')\n",
    "    os.system('export XLA_USE_BF16=1')\n",
    "    import torch_xla.core.xla_model as xm\n",
    "    import torch_xla.distributed.parallel_loader as pl\n",
    "    import torch_xla.distributed.xla_multiprocessing as xmp\n",
    "    CFG.lr = CFG.lr * CFG.nprocs\n",
    "    CFG.batch_size = CFG.batch_size // CFG.nprocs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.016314,
     "end_time": "2021-03-06T15:37:03.250155",
     "exception": false,
     "start_time": "2021-03-06T15:37:03.233841",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-06T15:37:03.297474Z",
     "iopub.status.busy": "2021-03-06T15:37:03.296782Z",
     "iopub.status.idle": "2021-03-06T15:37:06.683561Z",
     "shell.execute_reply": "2021-03-06T15:37:06.684349Z"
    },
    "papermill": {
     "duration": 3.416293,
     "end_time": "2021-03-06T15:37:06.684584",
     "exception": false,
     "start_time": "2021-03-06T15:37:03.268291",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Library\n",
    "# ====================================================\n",
    "import sys\n",
    "sys.path.append('/home/yuki/RANZCR/input/pytorch-image-models/pytorch-image-models-master')\n",
    "\n",
    "import os\n",
    "import ast\n",
    "import copy\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from contextlib import contextmanager\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from functools import partial\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, SGD\n",
    "import torchvision.models as models\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CosineAnnealingLR, ReduceLROnPlateau\n",
    "\n",
    "from albumentations import (\n",
    "    Compose, OneOf, Normalize, Resize, RandomResizedCrop, RandomCrop, HorizontalFlip, VerticalFlip, \n",
    "    RandomBrightness, RandomContrast, RandomBrightnessContrast, Rotate, ShiftScaleRotate, Cutout, \n",
    "    IAAAdditiveGaussianNoise, Transpose, HueSaturationValue, CoarseDropout\n",
    "    )\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from albumentations import ImageOnlyTransform\n",
    "\n",
    "import timm\n",
    "\n",
    "if CFG.device == 'TPU':\n",
    "    import ignite.distributed as idist\n",
    "elif CFG.device == 'GPU':\n",
    "    from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.016935,
     "end_time": "2021-03-06T15:37:06.725186",
     "exception": false,
     "start_time": "2021-03-06T15:37:06.708251",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-06T15:37:06.767926Z",
     "iopub.status.busy": "2021-03-06T15:37:06.767286Z",
     "iopub.status.idle": "2021-03-06T15:37:06.777705Z",
     "shell.execute_reply": "2021-03-06T15:37:06.777000Z"
    },
    "papermill": {
     "duration": 0.035712,
     "end_time": "2021-03-06T15:37:06.777859",
     "exception": false,
     "start_time": "2021-03-06T15:37:06.742147",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Utils\n",
    "# ====================================================\n",
    "def get_score(y_true, y_pred):\n",
    "    scores = []\n",
    "    for i in range(y_true.shape[1]):\n",
    "        score = roc_auc_score(y_true[:, i], y_pred[:, i])\n",
    "        scores.append(score)\n",
    "    avg_score = np.mean(scores)\n",
    "    return avg_score, scores\n",
    "\n",
    "@contextmanager\n",
    "def timer(name):\n",
    "    t0 = time.time()\n",
    "    LOGGER.info(f'[{name}] start')\n",
    "    yield\n",
    "    LOGGER.info(f'[{name}] done in {time.time() - t0:.0f}')\n",
    "    \n",
    "def init_logger(log_file=OUTPUT_DIR+'train.log'):\n",
    "    from logging import getLogger, INFO, FileHandler,  Formatter,  StreamHandler\n",
    "    logger = getLogger(__name__)\n",
    "    logger.setLevel(INFO)\n",
    "    handler1 = StreamHandler()\n",
    "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
    "    handler2 = FileHandler(filename=log_file)\n",
    "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    return logger\n",
    "\n",
    "LOGGER = init_logger()\n",
    "\n",
    "def seed_torch(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed_torch(seed=CFG.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.017652,
     "end_time": "2021-03-06T15:37:06.813488",
     "exception": false,
     "start_time": "2021-03-06T15:37:06.795836",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## CV splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-06T15:37:06.855589Z",
     "iopub.status.busy": "2021-03-06T15:37:06.854520Z",
     "iopub.status.idle": "2021-03-06T15:37:06.944332Z",
     "shell.execute_reply": "2021-03-06T15:37:06.943761Z"
    },
    "papermill": {
     "duration": 0.113747,
     "end_time": "2021-03-06T15:37:06.944474",
     "exception": false,
     "start_time": "2021-03-06T15:37:06.830727",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fold\n",
       "0    6017\n",
       "1    6017\n",
       "2    6016\n",
       "3    6016\n",
       "4    6016\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "folds = train.copy()\n",
    "Fold = GroupKFold(n_splits=CFG.n_fold)\n",
    "groups = folds['PatientID'].values\n",
    "for n, (train_index, val_index) in enumerate(Fold.split(folds, folds[CFG.target_cols], groups)):\n",
    "    folds.loc[val_index, 'fold'] = int(n)\n",
    "folds['fold'] = folds['fold'].astype(int)\n",
    "print(folds.groupby('fold').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-06T15:37:06.991667Z",
     "iopub.status.busy": "2021-03-06T15:37:06.990964Z",
     "iopub.status.idle": "2021-03-06T15:37:06.994382Z",
     "shell.execute_reply": "2021-03-06T15:37:06.993757Z"
    },
    "papermill": {
     "duration": 0.032196,
     "end_time": "2021-03-06T15:37:06.994521",
     "exception": false,
     "start_time": "2021-03-06T15:37:06.962325",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Dataset\n",
    "# ====================================================\n",
    "COLOR_MAP = {'ETT - Abnormal': (255, 0, 0),\n",
    "             'ETT - Borderline': (0, 255, 0),\n",
    "             'ETT - Normal': (0, 0, 255),\n",
    "             'NGT - Abnormal': (255, 255, 0),\n",
    "             'NGT - Borderline': (255, 0, 255),\n",
    "             'NGT - Incompletely Imaged': (0, 255, 255),\n",
    "             'NGT - Normal': (128, 0, 0),\n",
    "             'CVC - Abnormal': (0, 128, 0),\n",
    "             'CVC - Borderline': (0, 0, 128),\n",
    "             'CVC - Normal': (128, 128, 0),\n",
    "             'Swan Ganz Catheter Present': (128, 0, 128),\n",
    "            }\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df\n",
    "        self.file_names = df['StudyInstanceUID'].values\n",
    "        self.labels = df[CFG.target_cols].values\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_name = self.file_names[idx]\n",
    "        file_path = f'{TRAIN_PATH}/{file_name}.jpg'\n",
    "        image = cv2.imread(file_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image)\n",
    "            image = augmented['image']\n",
    "        label = torch.tensor(self.labels[idx]).float()\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.01757,
     "end_time": "2021-03-06T15:37:07.029961",
     "exception": false,
     "start_time": "2021-03-06T15:37:07.012391",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-06T15:37:07.076390Z",
     "iopub.status.busy": "2021-03-06T15:37:07.075670Z",
     "iopub.status.idle": "2021-03-06T15:37:07.079448Z",
     "shell.execute_reply": "2021-03-06T15:37:07.078892Z"
    },
    "papermill": {
     "duration": 0.031825,
     "end_time": "2021-03-06T15:37:07.079599",
     "exception": false,
     "start_time": "2021-03-06T15:37:07.047774",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Transforms\n",
    "# ====================================================\n",
    "def get_transforms(*, data):\n",
    "    \n",
    "    if data == 'train':\n",
    "        return Compose([\n",
    "            #Resize(CFG.size, CFG.size),\n",
    "            RandomResizedCrop(CFG.size, CFG.size, scale=(0.85, 1.0)),\n",
    "            HorizontalFlip(p=0.5),\n",
    "            RandomBrightnessContrast(p=0.2, brightness_limit=(-0.2, 0.2), contrast_limit=(-0.2, 0.2)),\n",
    "            HueSaturationValue(p=0.2, hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2),\n",
    "            ShiftScaleRotate(p=0.2, shift_limit=0.0625, scale_limit=0.2, rotate_limit=20),\n",
    "            CoarseDropout(p=0.2),\n",
    "            Cutout(p=0.2, max_h_size=16, max_w_size=16, fill_value=(0., 0., 0.), num_holes=16),\n",
    "            Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225],\n",
    "            ),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "    \n",
    "    elif data == 'valid':\n",
    "        return Compose([\n",
    "            Resize(CFG.size, CFG.size),\n",
    "            Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225],\n",
    "            ),\n",
    "            ToTensorV2(),\n",
    "\n",
    "        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.028522,
     "end_time": "2021-03-06T15:37:08.622426",
     "exception": false,
     "start_time": "2021-03-06T15:37:08.593904",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-06T15:37:08.690078Z",
     "iopub.status.busy": "2021-03-06T15:37:08.689385Z",
     "iopub.status.idle": "2021-03-06T15:37:08.691510Z",
     "shell.execute_reply": "2021-03-06T15:37:08.692076Z"
    },
    "papermill": {
     "duration": 0.040947,
     "end_time": "2021-03-06T15:37:08.692275",
     "exception": false,
     "start_time": "2021-03-06T15:37:08.651328",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# MODEL\n",
    "# ====================================================\n",
    "class CustomResNet200D(nn.Module):\n",
    "    def __init__(self, model_name='resnet200d_320', pretrained=False):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model(model_name, pretrained=False)\n",
    "        if pretrained:\n",
    "            pretrained_path = '../input/resnet200d-pretrained-weight/resnet200d_ra2-bdba9bf9.pth'\n",
    "            self.model.load_state_dict(torch.load(pretrained_path))\n",
    "            print(f'load {model_name} pretrained model')\n",
    "        n_features = self.model.fc.in_features\n",
    "        self.model.global_pool = nn.Identity()\n",
    "        self.model.fc = nn.Identity()\n",
    "        self.pooling = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Linear(n_features, CFG.target_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        bs = x.size(0)\n",
    "        features = self.model(x)\n",
    "        pooled_features = self.pooling(features).view(bs, -1)\n",
    "        output = self.fc(pooled_features)\n",
    "        return features, pooled_features, output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.03038,
     "end_time": "2021-03-06T15:37:08.751995",
     "exception": false,
     "start_time": "2021-03-06T15:37:08.721615",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-06T15:37:08.814485Z",
     "iopub.status.busy": "2021-03-06T15:37:08.813791Z",
     "iopub.status.idle": "2021-03-06T15:37:08.846683Z",
     "shell.execute_reply": "2021-03-06T15:37:08.846125Z"
    },
    "papermill": {
     "duration": 0.065168,
     "end_time": "2021-03-06T15:37:08.846834",
     "exception": false,
     "start_time": "2021-03-06T15:37:08.781666",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Helper functions\n",
    "# ====================================================\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "        \n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "def train_fn(train_loader, model, criterion, optimizer, epoch, scheduler, device):\n",
    "    if CFG.device == 'GPU':\n",
    "        scaler = GradScaler()\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    scores = AverageMeter()\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "    start = end = time.time()\n",
    "    global_step = 0\n",
    "    for step, (images, labels) in enumerate(train_loader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = labels.size(0)\n",
    "        if CFG.device == 'GPU':\n",
    "            with autocast():\n",
    "                _, _, y_preds = model(images)\n",
    "                loss = criterion(y_preds, labels)\n",
    "                # record loss\n",
    "                losses.update(loss.item(), batch_size)\n",
    "                if CFG.gradient_accumulation_steps > 1:\n",
    "                    loss = loss / CFG.gradient_accumulation_steps\n",
    "                scaler.scale(loss).backward()\n",
    "                grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n",
    "                if (step + 1) % CFG.gradient_accumulation_steps == 0:\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                    optimizer.zero_grad()\n",
    "                    global_step += 1\n",
    "                    \n",
    "        elif CFG.device == 'TPU':\n",
    "            _, _, y_preds = model(images)\n",
    "            loss = criterion(y_preds, labels)\n",
    "            # record loss\n",
    "            losses.update(loss.item(), batch_size)\n",
    "            if CFG.gradient_accumulation_steps > 1:\n",
    "                loss = loss / CFG.gradient_accumulation_steps\n",
    "            loss.backward()\n",
    "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n",
    "            if (step + 1) % CFG.gradient_accumulation_steps == 0:\n",
    "                xm.optimizer_step(optimizer, barrier=True)\n",
    "                optimizer.zero_grad()\n",
    "                global_step += 1\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        if CFG.device == 'GPU':\n",
    "            if step % CFG.print_freq == 0 or step == (len(train_loader)-1):\n",
    "                print('Epoch: [{0}][{1}/{2}] '\n",
    "                      'Data {data_time.val:.3f} ({data_time.avg:.3f}) '\n",
    "                      'Elapsed {remain:s} '\n",
    "                      'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                      'Grad: {grad_norm:.4f}  '\n",
    "                      #'LR: {lr:.6f}  '\n",
    "                      .format(\n",
    "                       epoch+1, step, len(train_loader), batch_time=batch_time,\n",
    "                       data_time=data_time, loss=losses,\n",
    "                       remain=timeSince(start, float(step+1)/len(train_loader)),\n",
    "                       grad_norm=grad_norm,\n",
    "                       #lr=scheduler.get_lr()[0],\n",
    "                       ))\n",
    "        elif CFG.device == 'TPU':\n",
    "            if step % CFG.print_freq == 0 or step == (len(train_loader)-1):\n",
    "                xm.master_print('Epoch: [{0}][{1}/{2}] '\n",
    "                                'Data {data_time.val:.3f} ({data_time.avg:.3f}) '\n",
    "                                'Elapsed {remain:s} '\n",
    "                                'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                                'Grad: {grad_norm:.4f}  '\n",
    "                                #'LR: {lr:.6f}  '\n",
    "                                .format(\n",
    "                                epoch+1, step, len(train_loader), batch_time=batch_time,\n",
    "                                data_time=data_time, loss=losses,\n",
    "                                remain=timeSince(start, float(step+1)/len(train_loader)),\n",
    "                                grad_norm=grad_norm,\n",
    "                                #lr=scheduler.get_lr()[0],\n",
    "                                ))\n",
    "    return losses.avg\n",
    "\n",
    "def valid_fn(valid_loader, model, criterion, device):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    scores = AverageMeter()\n",
    "    # switch to evaluation mode\n",
    "    model.eval()\n",
    "    trues = []\n",
    "    preds = []\n",
    "    start = end = time.time()\n",
    "    for step, (images, labels) in enumerate(valid_loader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = labels.size(0)\n",
    "        # compute loss\n",
    "        with torch.no_grad():\n",
    "            _, _, y_preds = model(images)\n",
    "        loss = criterion(y_preds, labels)\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        # record accuracy\n",
    "        trues.append(labels.to('cpu').numpy())\n",
    "        preds.append(y_preds.sigmoid().to('cpu').numpy())\n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        if CFG.device == 'GPU':\n",
    "            if step % CFG.print_freq == 0 or step == (len(valid_loader)-1):\n",
    "                print('EVAL: [{0}/{1}] '\n",
    "                      'Data {data_time.val:.3f} ({data_time.avg:.3f}) '\n",
    "                      'Elapsed {remain:s} '\n",
    "                      'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                      .format(\n",
    "                       step, len(valid_loader), batch_time=batch_time,\n",
    "                       data_time=data_time, loss=losses,\n",
    "                       remain=timeSince(start, float(step+1)/len(valid_loader)),\n",
    "                       ))\n",
    "        elif CFG.device == 'TPU':\n",
    "            if step % CFG.print_freq == 0 or step == (len(valid_loader)-1):\n",
    "                xm.master_print('EVAL: [{0}/{1}] '\n",
    "                                'Data {data_time.val:.3f} ({data_time.avg:.3f}) '\n",
    "                                'Elapsed {remain:s} '\n",
    "                                'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                                .format(\n",
    "                                step, len(valid_loader), batch_time=batch_time,\n",
    "                                data_time=data_time, loss=losses,\n",
    "                                remain=timeSince(start, float(step+1)/len(valid_loader)),\n",
    "                                ))\n",
    "    trues = np.concatenate(trues)\n",
    "    predictions = np.concatenate(preds)\n",
    "    return losses.avg, predictions, trues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.02859,
     "end_time": "2021-03-06T15:37:08.904437",
     "exception": false,
     "start_time": "2021-03-06T15:37:08.875847",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-06T15:37:09.002852Z",
     "iopub.status.busy": "2021-03-06T15:37:08.972045Z",
     "iopub.status.idle": "2021-03-06T15:37:09.005868Z",
     "shell.execute_reply": "2021-03-06T15:37:09.005238Z"
    },
    "papermill": {
     "duration": 0.07244,
     "end_time": "2021-03-06T15:37:09.006019",
     "exception": false,
     "start_time": "2021-03-06T15:37:08.933579",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Train loop\n",
    "# ====================================================\n",
    "def train_loop(folds, fold):\n",
    "\n",
    "    if CFG.device == 'GPU':\n",
    "        LOGGER.info(f\"========== fold: {fold} training ==========\")\n",
    "    elif CFG.device == 'TPU':\n",
    "        if CFG.nprocs == 1:\n",
    "            LOGGER.info(f\"========== fold: {fold} training ==========\")\n",
    "        elif CFG.nprocs == 8:\n",
    "            xm.master_print(f\"========== fold: {fold} training ==========\")\n",
    "            \n",
    "    # ====================================================\n",
    "    # loader\n",
    "    # ====================================================\n",
    "    trn_idx = folds[folds['fold'] != fold].index\n",
    "    val_idx = folds[folds['fold'] == fold].index\n",
    "\n",
    "    train_folds = folds.loc[trn_idx].reset_index(drop=True)\n",
    "    valid_folds = folds.loc[val_idx].reset_index(drop=True)\n",
    "    valid_labels = valid_folds[CFG.target_cols].values\n",
    "\n",
    "    train_dataset = TrainDataset(train_folds, \n",
    "                                 transform=get_transforms(data='train'))\n",
    "    valid_dataset = TrainDataset(valid_folds, \n",
    "                                 transform=get_transforms(data='valid'))\n",
    "    \n",
    "    if CFG.device == 'GPU':\n",
    "        train_loader = DataLoader(train_dataset, \n",
    "                                  batch_size=CFG.batch_size, \n",
    "                                  shuffle=True, \n",
    "                                  num_workers=CFG.num_workers, pin_memory=True, drop_last=True)\n",
    "        valid_loader = DataLoader(valid_dataset, \n",
    "                                  batch_size=CFG.batch_size * 2, \n",
    "                                  shuffle=False, \n",
    "                                  num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n",
    "        \n",
    "    elif CFG.device == 'TPU':\n",
    "        train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset,\n",
    "                                                                        num_replicas=xm.xrt_world_size(),\n",
    "                                                                        rank=xm.get_ordinal(),\n",
    "                                                                        shuffle=True)\n",
    "        train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                                   batch_size=CFG.batch_size,\n",
    "                                                   sampler=train_sampler,\n",
    "                                                   drop_last=True,\n",
    "                                                   num_workers=CFG.num_workers)\n",
    "        \n",
    "        valid_sampler = torch.utils.data.distributed.DistributedSampler(valid_dataset,\n",
    "                                                                        num_replicas=xm.xrt_world_size(),\n",
    "                                                                        rank=xm.get_ordinal(),\n",
    "                                                                        shuffle=False)\n",
    "        valid_loader = torch.utils.data.DataLoader(valid_dataset,\n",
    "                                                   batch_size=CFG.batch_size * 2,\n",
    "                                                   sampler=valid_sampler,\n",
    "                                                   drop_last=False,\n",
    "                                                   num_workers=CFG.num_workers)\n",
    "        \n",
    "    # ====================================================\n",
    "    # scheduler \n",
    "    # ====================================================\n",
    "    def get_scheduler(optimizer):\n",
    "        if CFG.scheduler=='ReduceLROnPlateau':\n",
    "            scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=CFG.factor, patience=CFG.patience, verbose=True, eps=CFG.eps)\n",
    "        elif CFG.scheduler=='CosineAnnealingLR':\n",
    "            scheduler = CosineAnnealingLR(optimizer, T_max=CFG.T_max, eta_min=CFG.min_lr, last_epoch=-1)\n",
    "        elif CFG.scheduler=='CosineAnnealingWarmRestarts':\n",
    "            scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=CFG.T_0, T_mult=1, eta_min=CFG.min_lr, last_epoch=-1)\n",
    "        return scheduler\n",
    "    \n",
    "    # ====================================================\n",
    "    # model & optimizer\n",
    "    # ====================================================\n",
    "    if CFG.device == 'TPU':\n",
    "        device = xm.xla_device()\n",
    "    elif CFG.device == 'GPU':\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    model = CustomResNet200D(CFG.model_name, pretrained=False)\n",
    "    model.load_state_dict(torch.load(CFG.student, map_location=torch.device('cpu'))['model'])\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = Adam(model.parameters(), lr=CFG.lr, weight_decay=CFG.weight_decay, amsgrad=False)\n",
    "    scheduler = get_scheduler(optimizer)\n",
    "    \n",
    "    # ====================================================\n",
    "    # loop\n",
    "    # ====================================================\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    best_score = 0.\n",
    "    best_loss = np.inf\n",
    "    \n",
    "    for epoch in range(CFG.epochs):\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # train\n",
    "        if CFG.device == 'TPU':\n",
    "            if CFG.nprocs == 1:\n",
    "                avg_loss = train_fn(train_loader, model, criterion, optimizer, epoch, scheduler, device)\n",
    "            elif CFG.nprocs == 8:\n",
    "                para_train_loader = pl.ParallelLoader(train_loader, [device])\n",
    "                avg_loss = train_fn(para_train_loader.per_device_loader(device), model, criterion, optimizer, epoch, scheduler, device)\n",
    "        elif CFG.device == 'GPU':\n",
    "            avg_loss = train_fn(train_loader, model, criterion, optimizer, epoch, scheduler, device)\n",
    "        \n",
    "        # eval\n",
    "        if CFG.device == 'TPU':\n",
    "            if CFG.nprocs == 1:\n",
    "                avg_val_loss, preds, _ = valid_fn(valid_loader, model, criterion, device)\n",
    "            elif CFG.nprocs == 8:\n",
    "                para_valid_loader = pl.ParallelLoader(valid_loader, [device])\n",
    "                avg_val_loss, preds, valid_labels = valid_fn(para_valid_loader.per_device_loader(device), model, criterion, device)\n",
    "                preds = idist.all_gather(torch.tensor(preds)).to('cpu').numpy()\n",
    "                valid_labels = idist.all_gather(torch.tensor(valid_labels)).to('cpu').numpy()\n",
    "        elif CFG.device == 'GPU':\n",
    "            avg_val_loss, preds, _ = valid_fn(valid_loader, model, criterion, device)\n",
    "            \n",
    "        if isinstance(scheduler, ReduceLROnPlateau):\n",
    "            scheduler.step(avg_val_loss)\n",
    "        elif isinstance(scheduler, CosineAnnealingLR):\n",
    "            scheduler.step()\n",
    "        elif isinstance(scheduler, CosineAnnealingWarmRestarts):\n",
    "            scheduler.step()\n",
    "            \n",
    "        # scoring\n",
    "        score, scores = get_score(valid_labels, preds)\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "        \n",
    "        if CFG.device == 'GPU':\n",
    "            LOGGER.info(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n",
    "            LOGGER.info(f'Epoch {epoch+1} - Score: {score:.4f}  Scores: {np.round(scores, decimals=4)}')\n",
    "        elif CFG.device == 'TPU':\n",
    "            if CFG.nprocs == 1:\n",
    "                LOGGER.info(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n",
    "                LOGGER.info(f'Epoch {epoch+1} - Score: {score:.4f}  Scores: {np.round(scores, decimals=4)}')\n",
    "            elif CFG.nprocs == 8:\n",
    "                xm.master_print(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n",
    "                xm.master_print(f'Epoch {epoch+1} - Score: {score:.4f}  Scores: {np.round(scores, decimals=4)}')\n",
    "        \n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            if CFG.device == 'GPU':\n",
    "                LOGGER.info(f'Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model')\n",
    "                torch.save({'model': model.state_dict(), \n",
    "                            'preds': preds},\n",
    "                           OUTPUT_DIR+f'{CFG.model_name}_fold{fold}_best_score.pth')\n",
    "            elif CFG.device == 'TPU':\n",
    "                if CFG.nprocs == 1:\n",
    "                    LOGGER.info(f'Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model')\n",
    "                elif CFG.nprocs == 8:\n",
    "                    xm.master_print(f'Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model')\n",
    "                xm.save({'model': model, \n",
    "                         'preds': preds}, \n",
    "                        OUTPUT_DIR+f'{CFG.model_name}_fold{fold}_best_score.pth')\n",
    "                \n",
    "        if avg_val_loss < best_loss:\n",
    "            best_loss = avg_val_loss\n",
    "            if CFG.device == 'GPU':\n",
    "                LOGGER.info(f'Epoch {epoch+1} - Save Best Loss: {best_loss:.4f} Model')\n",
    "                torch.save({'model': model.state_dict(), \n",
    "                            'preds': preds},\n",
    "                           OUTPUT_DIR+f'{CFG.model_name}_fold{fold}_best_loss.pth')\n",
    "            elif CFG.device == 'TPU':\n",
    "                if CFG.nprocs == 1:\n",
    "                    LOGGER.info(f'Epoch {epoch+1} - Save Best Loss: {best_loss:.4f} Model')\n",
    "                elif CFG.nprocs == 8:\n",
    "                    xm.master_print(f'Epoch {epoch+1} - Save Best Loss: {best_loss:.4f} Model')\n",
    "                xm.save({'model': model, \n",
    "                         'preds': preds}, \n",
    "                        OUTPUT_DIR+f'{CFG.model_name}_fold{fold}_best_loss.pth')\n",
    "        \n",
    "        # inference用に全て保存しておく\n",
    "        if CFG.device == 'TPU':\n",
    "            xm.save({'model': model}, OUTPUT_DIR+f'{CFG.model_name}_fold{fold}_epoch{epoch+1}.pth')\n",
    "        elif CFG.device == 'GPU':\n",
    "            torch.save({'model': model.state_dict()}, OUTPUT_DIR+f'{CFG.model_name}_fold{fold}_epoch{epoch+1}.pth')\n",
    "        \n",
    "        if CFG.nprocs != 8:\n",
    "            check_point = torch.load(OUTPUT_DIR+f'{CFG.model_name}_fold{fold}_best_score.pth')\n",
    "            for c in [f'pred_{c}' for c in CFG.target_cols]:\n",
    "                valid_folds[c] = np.nan\n",
    "            valid_folds[[f'pred_{c}' for c in CFG.target_cols]] = check_point['preds']\n",
    "\n",
    "    return valid_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-06T15:37:09.074178Z",
     "iopub.status.busy": "2021-03-06T15:37:09.073460Z",
     "iopub.status.idle": "2021-03-06T15:37:09.075640Z",
     "shell.execute_reply": "2021-03-06T15:37:09.076112Z"
    },
    "papermill": {
     "duration": 0.040889,
     "end_time": "2021-03-06T15:37:09.076283",
     "exception": false,
     "start_time": "2021-03-06T15:37:09.035394",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# main\n",
    "# ====================================================\n",
    "def main():\n",
    "\n",
    "    \"\"\"\n",
    "    Prepare: 1.train  2.folds\n",
    "    \"\"\"\n",
    "\n",
    "    def get_result(result_df):\n",
    "        preds = result_df[[f'pred_{c}' for c in CFG.target_cols]].values\n",
    "        labels = result_df[CFG.target_cols].values\n",
    "        score, scores = get_score(labels, preds)\n",
    "        LOGGER.info(f'Score: {score:<.4f}  Scores: {np.round(scores, decimals=4)}')\n",
    "        \n",
    "    if CFG.train:\n",
    "        # train \n",
    "        oof_df = pd.DataFrame()\n",
    "        for fold in range(CFG.n_fold):\n",
    "            if fold in CFG.trn_fold:\n",
    "                _oof_df = train_loop(folds, fold)\n",
    "                oof_df = pd.concat([oof_df, _oof_df])\n",
    "                if CFG.nprocs != 8:\n",
    "                    LOGGER.info(f\"========== fold: {fold} result ==========\")\n",
    "                    get_result(_oof_df)\n",
    "                    \n",
    "        if CFG.nprocs != 8:\n",
    "            # CV result\n",
    "            LOGGER.info(f\"========== CV ==========\")\n",
    "            get_result(oof_df)\n",
    "            # save result\n",
    "            oof_df.to_csv(OUTPUT_DIR+'oof_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-06T15:37:09.138498Z",
     "iopub.status.busy": "2021-03-06T15:37:09.137798Z",
     "iopub.status.idle": "2021-03-06T19:05:47.882957Z",
     "shell.execute_reply": "2021-03-06T19:05:47.884136Z"
    },
    "papermill": {
     "duration": 12518.778621,
     "end_time": "2021-03-06T19:05:47.884442",
     "exception": false,
     "start_time": "2021-03-06T15:37:09.105821",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 1 training ==========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/1504] Data 1.994 (1.994) Elapsed 1m 19s (remain 1986m 53s) Loss: 0.0713(0.0713) Grad: 0.5428  \n",
      "Epoch: [1][100/1504] Data 0.019 (0.046) Elapsed 4m 59s (remain 69m 24s) Loss: 0.1233(0.1285) Grad: 0.6004  \n",
      "Epoch: [1][200/1504] Data 0.017 (0.037) Elapsed 7m 19s (remain 47m 28s) Loss: 0.1473(0.1196) Grad: 0.7587  \n",
      "Epoch: [1][300/1504] Data 0.018 (0.034) Elapsed 9m 36s (remain 38m 25s) Loss: 0.1413(0.1171) Grad: 0.8039  \n",
      "Epoch: [1][400/1504] Data 0.016 (0.032) Elapsed 11m 53s (remain 32m 43s) Loss: 0.0992(0.1176) Grad: 0.6312  \n",
      "Epoch: [1][500/1504] Data 0.016 (0.031) Elapsed 14m 10s (remain 28m 23s) Loss: 0.0748(0.1173) Grad: 0.4402  \n",
      "Epoch: [1][600/1504] Data 0.018 (0.030) Elapsed 16m 27s (remain 24m 44s) Loss: 0.0723(0.1168) Grad: 0.6617  \n",
      "Epoch: [1][700/1504] Data 0.018 (0.030) Elapsed 18m 48s (remain 21m 32s) Loss: 0.0589(0.1173) Grad: 0.3692  \n",
      "Epoch: [1][800/1504] Data 0.018 (0.030) Elapsed 21m 6s (remain 18m 31s) Loss: 0.1049(0.1174) Grad: 0.5952  \n",
      "Epoch: [1][900/1504] Data 0.017 (0.030) Elapsed 23m 23s (remain 15m 39s) Loss: 0.1350(0.1175) Grad: 0.5531  \n",
      "Epoch: [1][1000/1504] Data 0.018 (0.029) Elapsed 25m 40s (remain 12m 54s) Loss: 0.1451(0.1173) Grad: 1.0584  \n",
      "Epoch: [1][1100/1504] Data 0.019 (0.029) Elapsed 28m 1s (remain 10m 15s) Loss: 0.1049(0.1167) Grad: 0.5780  \n",
      "Epoch: [1][1200/1504] Data 0.019 (0.029) Elapsed 30m 19s (remain 7m 38s) Loss: 0.1336(0.1164) Grad: 0.5873  \n",
      "Epoch: [1][1300/1504] Data 0.021 (0.029) Elapsed 32m 36s (remain 5m 5s) Loss: 0.0923(0.1161) Grad: 0.6309  \n",
      "Epoch: [1][1400/1504] Data 0.018 (0.029) Elapsed 34m 54s (remain 2m 33s) Loss: 0.0625(0.1155) Grad: 0.5340  \n",
      "Epoch: [1][1500/1504] Data 0.019 (0.029) Elapsed 37m 11s (remain 0m 4s) Loss: 0.0863(0.1148) Grad: 0.6300  \n",
      "Epoch: [1][1503/1504] Data 0.017 (0.029) Elapsed 37m 16s (remain 0m 0s) Loss: 0.1328(0.1148) Grad: 0.7411  \n",
      "EVAL: [0/189] Data 4.054 (4.054) Elapsed 0m 23s (remain 74m 40s) Loss: 0.1013(0.1013) \n",
      "EVAL: [100/189] Data 0.030 (0.073) Elapsed 3m 8s (remain 2m 44s) Loss: 0.1144(0.1158) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 0.1148  avg_val_loss: 0.1167  time: 2576s\n",
      "Epoch 1 - Score: 0.9677  Scores: [0.9804 0.9679 0.9932 0.9709 0.9769 0.9876 0.9889 0.949  0.8989 0.9312\n",
      " 0.9995]\n",
      "Epoch 1 - Save Best Score: 0.9677 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [188/189] Data 0.025 (0.053) Elapsed 5m 39s (remain 0m 0s) Loss: 0.5681(0.1167) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Save Best Loss: 0.1167 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][0/1504] Data 2.008 (2.008) Elapsed 1m 43s (remain 2604m 28s) Loss: 0.0456(0.0456) Grad: 0.3178  \n",
      "Epoch: [2][100/1504] Data 0.016 (0.048) Elapsed 4m 4s (remain 56m 38s) Loss: 0.1274(0.1152) Grad: 0.7037  \n",
      "Epoch: [2][200/1504] Data 0.019 (0.038) Elapsed 6m 28s (remain 42m 0s) Loss: 0.1434(0.1083) Grad: 0.7529  \n",
      "Epoch: [2][300/1504] Data 0.018 (0.035) Elapsed 8m 48s (remain 35m 12s) Loss: 0.1697(0.1078) Grad: 0.6808  \n",
      "Epoch: [2][400/1504] Data 0.017 (0.033) Elapsed 11m 8s (remain 30m 39s) Loss: 0.1157(0.1085) Grad: 0.7742  \n",
      "Epoch: [2][500/1504] Data 0.018 (0.032) Elapsed 13m 28s (remain 26m 58s) Loss: 0.0750(0.1086) Grad: 0.4279  \n",
      "Epoch: [2][600/1504] Data 0.019 (0.032) Elapsed 15m 52s (remain 23m 50s) Loss: 0.0637(0.1086) Grad: 0.4494  \n",
      "Epoch: [2][700/1504] Data 0.019 (0.031) Elapsed 18m 11s (remain 20m 50s) Loss: 0.0650(0.1093) Grad: 0.3652  \n",
      "Epoch: [2][800/1504] Data 0.020 (0.031) Elapsed 20m 31s (remain 18m 1s) Loss: 0.0808(0.1097) Grad: 0.4941  \n",
      "Epoch: [2][900/1504] Data 0.018 (0.031) Elapsed 22m 51s (remain 15m 18s) Loss: 0.1305(0.1099) Grad: 0.7378  \n",
      "Epoch: [2][1000/1504] Data 0.020 (0.030) Elapsed 25m 12s (remain 12m 39s) Loss: 0.1642(0.1098) Grad: 0.7793  \n",
      "Epoch: [2][1100/1504] Data 0.020 (0.030) Elapsed 27m 36s (remain 10m 6s) Loss: 0.0921(0.1096) Grad: 0.5612  \n",
      "Epoch: [2][1200/1504] Data 0.021 (0.030) Elapsed 29m 56s (remain 7m 33s) Loss: 0.1108(0.1091) Grad: 0.6102  \n",
      "Epoch: [2][1300/1504] Data 0.019 (0.030) Elapsed 32m 17s (remain 5m 2s) Loss: 0.0883(0.1088) Grad: 0.6804  \n",
      "Epoch: [2][1400/1504] Data 0.019 (0.030) Elapsed 34m 37s (remain 2m 32s) Loss: 0.0716(0.1083) Grad: 0.5319  \n",
      "Epoch: [2][1500/1504] Data 0.019 (0.030) Elapsed 37m 1s (remain 0m 4s) Loss: 0.1027(0.1077) Grad: 0.6424  \n",
      "Epoch: [2][1503/1504] Data 0.018 (0.031) Elapsed 37m 6s (remain 0m 0s) Loss: 0.1154(0.1077) Grad: 0.7068  \n",
      "EVAL: [0/189] Data 4.562 (4.562) Elapsed 0m 7s (remain 22m 34s) Loss: 0.1080(0.1080) \n",
      "EVAL: [100/189] Data 0.044 (0.076) Elapsed 2m 42s (remain 2m 21s) Loss: 0.1149(0.1175) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 0.1077  avg_val_loss: 0.1185  time: 2524s\n",
      "Epoch 2 - Score: 0.9679  Scores: [0.9859 0.9675 0.9936 0.9718 0.9753 0.9872 0.9881 0.9499 0.8976 0.9305\n",
      " 0.9994]\n",
      "Epoch 2 - Save Best Score: 0.9679 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [188/189] Data 0.026 (0.054) Elapsed 4m 57s (remain 0m 0s) Loss: 0.2353(0.1185) \n",
      "Epoch: [3][0/1504] Data 2.138 (2.138) Elapsed 0m 3s (remain 93m 12s) Loss: 0.0321(0.0321) Grad: 0.2430  \n",
      "Epoch: [3][100/1504] Data 0.017 (0.048) Elapsed 2m 23s (remain 33m 17s) Loss: 0.1032(0.1108) Grad: 0.8187  \n",
      "Epoch: [3][200/1504] Data 0.020 (0.039) Elapsed 4m 47s (remain 31m 2s) Loss: 0.2013(0.1052) Grad: 1.0057  \n",
      "Epoch: [3][300/1504] Data 0.018 (0.035) Elapsed 7m 7s (remain 28m 27s) Loss: 0.1666(0.1030) Grad: 0.7161  \n",
      "Epoch: [3][400/1504] Data 0.019 (0.034) Elapsed 9m 27s (remain 26m 1s) Loss: 0.0839(0.1036) Grad: 0.5545  \n",
      "Epoch: [3][500/1504] Data 0.019 (0.032) Elapsed 11m 47s (remain 23m 36s) Loss: 0.0596(0.1038) Grad: 0.3640  \n",
      "Epoch: [3][600/1504] Data 0.029 (0.032) Elapsed 14m 9s (remain 21m 16s) Loss: 0.0873(0.1036) Grad: 0.7952  \n",
      "Epoch: [3][700/1504] Data 0.020 (0.031) Elapsed 16m 29s (remain 18m 53s) Loss: 0.0579(0.1040) Grad: 0.4743  \n",
      "Epoch: [3][800/1504] Data 0.018 (0.031) Elapsed 18m 49s (remain 16m 31s) Loss: 0.0882(0.1043) Grad: 0.5639  \n",
      "Epoch: [3][900/1504] Data 0.018 (0.031) Elapsed 21m 8s (remain 14m 9s) Loss: 0.0805(0.1044) Grad: 0.7012  \n",
      "Epoch: [3][1000/1504] Data 0.019 (0.030) Elapsed 23m 28s (remain 11m 47s) Loss: 0.1555(0.1044) Grad: 1.1651  \n",
      "Epoch: [3][1100/1504] Data 0.019 (0.030) Elapsed 25m 51s (remain 9m 28s) Loss: 0.1283(0.1041) Grad: 0.6478  \n",
      "Epoch: [3][1200/1504] Data 0.020 (0.030) Elapsed 28m 11s (remain 7m 6s) Loss: 0.0846(0.1037) Grad: 0.5898  \n",
      "Epoch: [3][1300/1504] Data 0.018 (0.030) Elapsed 30m 31s (remain 4m 45s) Loss: 0.0808(0.1034) Grad: 0.7211  \n",
      "Epoch: [3][1400/1504] Data 0.018 (0.030) Elapsed 32m 51s (remain 2m 24s) Loss: 0.0512(0.1030) Grad: 0.4723  \n",
      "Epoch: [3][1500/1504] Data 0.018 (0.030) Elapsed 35m 14s (remain 0m 4s) Loss: 0.0961(0.1021) Grad: 0.5560  \n",
      "Epoch: [3][1503/1504] Data 0.020 (0.031) Elapsed 35m 19s (remain 0m 0s) Loss: 0.1095(0.1022) Grad: 0.7936  \n",
      "EVAL: [0/189] Data 4.336 (4.336) Elapsed 0m 6s (remain 21m 41s) Loss: 0.1075(0.1075) \n",
      "EVAL: [100/189] Data 0.029 (0.073) Elapsed 2m 44s (remain 2m 23s) Loss: 0.1129(0.1184) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 0.1022  avg_val_loss: 0.1193  time: 2421s\n",
      "Epoch 3 - Score: 0.9677  Scores: [0.9848 0.969  0.9934 0.9719 0.9752 0.9873 0.988  0.948  0.8965 0.9309\n",
      " 0.9996]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [188/189] Data 0.027 (0.053) Elapsed 5m 0s (remain 0m 0s) Loss: 0.3385(0.1193) \n",
      "Epoch: [4][0/1504] Data 2.069 (2.069) Elapsed 0m 3s (remain 87m 43s) Loss: 0.0356(0.0356) Grad: 0.3557  \n",
      "Epoch: [4][100/1504] Data 0.017 (0.047) Elapsed 2m 22s (remain 33m 3s) Loss: 0.1101(0.1061) Grad: 0.8982  \n",
      "Epoch: [4][200/1504] Data 0.017 (0.038) Elapsed 4m 45s (remain 30m 51s) Loss: 0.1318(0.0997) Grad: 0.6972  \n",
      "Epoch: [4][300/1504] Data 0.019 (0.034) Elapsed 7m 4s (remain 28m 17s) Loss: 0.1474(0.0980) Grad: 0.8164  \n",
      "Epoch: [4][400/1504] Data 0.020 (0.033) Elapsed 9m 24s (remain 25m 52s) Loss: 0.0850(0.0985) Grad: 0.5454  \n",
      "Epoch: [4][500/1504] Data 0.019 (0.032) Elapsed 11m 44s (remain 23m 29s) Loss: 0.0628(0.0988) Grad: 0.4476  \n",
      "Epoch: [4][600/1504] Data 0.019 (0.031) Elapsed 14m 3s (remain 21m 7s) Loss: 0.0619(0.0981) Grad: 0.6097  \n",
      "Epoch: [4][700/1504] Data 0.020 (0.031) Elapsed 16m 26s (remain 18m 49s) Loss: 0.0560(0.0986) Grad: 0.4391  \n",
      "Epoch: [4][800/1504] Data 0.020 (0.031) Elapsed 18m 46s (remain 16m 28s) Loss: 0.1004(0.0989) Grad: 0.7439  \n",
      "Epoch: [4][900/1504] Data 0.020 (0.031) Elapsed 21m 6s (remain 14m 7s) Loss: 0.1052(0.0990) Grad: 0.8448  \n",
      "Epoch: [4][1000/1504] Data 0.019 (0.030) Elapsed 23m 26s (remain 11m 46s) Loss: 0.1356(0.0989) Grad: 0.8519  \n",
      "Epoch: [4][1100/1504] Data 0.019 (0.030) Elapsed 25m 48s (remain 9m 26s) Loss: 0.1148(0.0986) Grad: 0.8134  \n",
      "Epoch: [4][1200/1504] Data 0.019 (0.030) Elapsed 28m 7s (remain 7m 5s) Loss: 0.1021(0.0983) Grad: 0.7384  \n",
      "Epoch: [4][1300/1504] Data 0.018 (0.030) Elapsed 30m 27s (remain 4m 45s) Loss: 0.0758(0.0980) Grad: 0.6827  \n",
      "Epoch: [4][1400/1504] Data 0.019 (0.030) Elapsed 32m 46s (remain 2m 24s) Loss: 0.0593(0.0974) Grad: 0.6462  \n",
      "Epoch: [4][1500/1504] Data 0.018 (0.030) Elapsed 35m 4s (remain 0m 4s) Loss: 0.0852(0.0966) Grad: 0.9760  \n",
      "Epoch: [4][1503/1504] Data 0.018 (0.030) Elapsed 35m 9s (remain 0m 0s) Loss: 0.1306(0.0966) Grad: 1.0907  \n",
      "EVAL: [0/189] Data 4.161 (4.161) Elapsed 0m 7s (remain 24m 33s) Loss: 0.1066(0.1066) \n",
      "EVAL: [100/189] Data 0.032 (0.083) Elapsed 3m 0s (remain 2m 37s) Loss: 0.1140(0.1194) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 - avg_train_loss: 0.0966  avg_val_loss: 0.1202  time: 2428s\n",
      "Epoch 4 - Score: 0.9673  Scores: [0.9857 0.9689 0.9934 0.9704 0.9749 0.9872 0.9881 0.9467 0.8958 0.9301\n",
      " 0.9994]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [188/189] Data 0.028 (0.060) Elapsed 5m 17s (remain 0m 0s) Loss: 0.3220(0.1202) \n",
      "Epoch: [5][0/1504] Data 2.093 (2.093) Elapsed 0m 4s (remain 108m 41s) Loss: 0.0286(0.0286) Grad: 0.3638  \n",
      "Epoch: [5][100/1504] Data 0.018 (0.048) Elapsed 2m 27s (remain 34m 12s) Loss: 0.1217(0.1029) Grad: 0.9825  \n",
      "Epoch: [5][200/1504] Data 0.018 (0.037) Elapsed 4m 49s (remain 31m 19s) Loss: 0.1780(0.0956) Grad: 0.8052  \n",
      "Epoch: [5][300/1504] Data 0.021 (0.035) Elapsed 7m 16s (remain 29m 2s) Loss: 0.1412(0.0932) Grad: 1.2147  \n",
      "Epoch: [5][400/1504] Data 0.019 (0.033) Elapsed 9m 38s (remain 26m 30s) Loss: 0.0886(0.0938) Grad: 0.6692  \n",
      "Epoch: [5][500/1504] Data 0.019 (0.032) Elapsed 12m 0s (remain 24m 3s) Loss: 0.0704(0.0942) Grad: 0.5629  \n",
      "Epoch: [5][600/1504] Data 0.019 (0.032) Elapsed 14m 23s (remain 21m 37s) Loss: 0.0655(0.0942) Grad: 0.8421  \n",
      "Epoch: [5][700/1504] Data 0.019 (0.032) Elapsed 16m 49s (remain 19m 16s) Loss: 0.0440(0.0949) Grad: 0.3364  \n",
      "Epoch: [5][800/1504] Data 0.019 (0.031) Elapsed 19m 12s (remain 16m 51s) Loss: 0.1091(0.0947) Grad: 0.7004  \n",
      "Epoch: [5][900/1504] Data 0.020 (0.031) Elapsed 21m 35s (remain 14m 26s) Loss: 0.0824(0.0947) Grad: 0.9517  \n",
      "Epoch: [5][1000/1504] Data 0.018 (0.031) Elapsed 23m 57s (remain 12m 2s) Loss: 0.1087(0.0945) Grad: 0.9878  \n",
      "Epoch: [5][1100/1504] Data 0.020 (0.031) Elapsed 26m 24s (remain 9m 39s) Loss: 0.0952(0.0939) Grad: 0.7767  \n",
      "Epoch: [5][1200/1504] Data 0.019 (0.031) Elapsed 28m 47s (remain 7m 15s) Loss: 0.1132(0.0933) Grad: 0.7819  \n",
      "Epoch: [5][1300/1504] Data 0.020 (0.030) Elapsed 31m 9s (remain 4m 51s) Loss: 0.0726(0.0932) Grad: 0.9341  \n",
      "Epoch: [5][1400/1504] Data 0.019 (0.030) Elapsed 33m 32s (remain 2m 27s) Loss: 0.0479(0.0927) Grad: 0.6567  \n",
      "Epoch: [5][1500/1504] Data 0.019 (0.030) Elapsed 35m 55s (remain 0m 4s) Loss: 0.0648(0.0921) Grad: 0.6814  \n",
      "Epoch: [5][1503/1504] Data 0.019 (0.031) Elapsed 35m 59s (remain 0m 0s) Loss: 0.1090(0.0921) Grad: 0.9980  \n",
      "EVAL: [0/189] Data 4.453 (4.453) Elapsed 0m 7s (remain 22m 31s) Loss: 0.1008(0.1008) \n",
      "EVAL: [100/189] Data 0.031 (0.082) Elapsed 2m 57s (remain 2m 34s) Loss: 0.1151(0.1181) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 - avg_train_loss: 0.0921  avg_val_loss: 0.1189  time: 2475s\n",
      "Epoch 5 - Score: 0.9677  Scores: [0.9871 0.9689 0.9933 0.9706 0.9752 0.9871 0.9881 0.9473 0.8964 0.9316\n",
      " 0.9994]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [188/189] Data 0.028 (0.059) Elapsed 5m 13s (remain 0m 0s) Loss: 0.3016(0.1189) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 1 result ==========\n",
      "Score: 0.9679  Scores: [0.9859 0.9675 0.9936 0.9718 0.9753 0.9872 0.9881 0.9499 0.8976 0.9305\n",
      " 0.9994]\n",
      "========== CV ==========\n",
      "Score: 0.9679  Scores: [0.9859 0.9675 0.9936 0.9718 0.9753 0.9872 0.9881 0.9499 0.8976 0.9305\n",
      " 0.9994]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    if CFG.device == 'TPU':\n",
    "        def _mp_fn(rank, flags):\n",
    "            torch.set_default_tensor_type('torch.FloatTensor')\n",
    "            a = main()\n",
    "        FLAGS = {}\n",
    "        xmp.spawn(_mp_fn, args=(FLAGS,), nprocs=CFG.nprocs, start_method='fork')\n",
    "    elif CFG.device == 'GPU':\n",
    "        main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-06T19:05:48.036237Z",
     "iopub.status.busy": "2021-03-06T19:05:48.031364Z",
     "iopub.status.idle": "2021-03-06T19:06:56.468553Z",
     "shell.execute_reply": "2021-03-06T19:06:56.467939Z"
    },
    "papermill": {
     "duration": 68.51622,
     "end_time": "2021-03-06T19:06:56.468734",
     "exception": false,
     "start_time": "2021-03-06T19:05:47.952514",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save as cpu\n",
    "if CFG.device == 'TPU':\n",
    "    for fold in range(CFG.n_fold):\n",
    "        if fold in CFG.trn_fold:\n",
    "            # best score\n",
    "            state = torch.load(OUTPUT_DIR+f'{CFG.model_name}_fold{fold}_best_score.pth')\n",
    "            torch.save({'model': state['model'].to('cpu').state_dict(), \n",
    "                        'preds': state['preds']}, \n",
    "                        OUTPUT_DIR+f'{CFG.model_name}_fold{fold}_best_score_cpu.pth')\n",
    "            # best loss\n",
    "            state = torch.load(OUTPUT_DIR+f'{CFG.model_name}_fold{fold}_best_loss.pth')\n",
    "            torch.save({'model': state['model'].to('cpu').state_dict(), \n",
    "                        'preds': state['preds']}, \n",
    "                        OUTPUT_DIR+f'{CFG.model_name}_fold{fold}_best_loss_cpu.pth')\n",
    "            \n",
    "            for epoch in range(CFG.epochs):\n",
    "                state = torch.load(OUTPUT_DIR+f'{CFG.model_name}_fold{fold}_epoch{epoch+1}.pth')\n",
    "                torch.save({'model': state['model'].to('cpu').state_dict()}, \n",
    "                            OUTPUT_DIR+f'{CFG.model_name}_fold{fold}_{epoch+1}_cpu.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.068101,
     "end_time": "2021-03-06T19:06:56.603813",
     "exception": false,
     "start_time": "2021-03-06T19:06:56.535712",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 12668.163859,
   "end_time": "2021-03-06T19:06:58.190907",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-03-06T15:35:50.027048",
   "version": "2.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
