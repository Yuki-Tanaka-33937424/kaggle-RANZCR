{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.016976,
     "end_time": "2021-03-06T07:51:15.740325",
     "exception": false,
     "start_time": "2021-03-06T07:51:15.723349",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Directory settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-06T07:51:15.77792Z",
     "iopub.status.busy": "2021-03-06T07:51:15.777356Z",
     "iopub.status.idle": "2021-03-06T07:51:15.78183Z",
     "shell.execute_reply": "2021-03-06T07:51:15.781314Z"
    },
    "papermill": {
     "duration": 0.025378,
     "end_time": "2021-03-06T07:51:15.782019",
     "exception": false,
     "start_time": "2021-03-06T07:51:15.756641",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# Directory settings\n",
    "# ======================================================\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "OUTPUT_DIR = './'\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "    \n",
    "TRAIN_PATH = '/home/yuki/RANZCR/input/ranzcr-clip-catheter-line-classification/train'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.015561,
     "end_time": "2021-03-06T07:51:15.813565",
     "exception": false,
     "start_time": "2021-03-06T07:51:15.798004",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-06T07:51:15.851872Z",
     "iopub.status.busy": "2021-03-06T07:51:15.851334Z",
     "iopub.status.idle": "2021-03-06T07:51:16.098743Z",
     "shell.execute_reply": "2021-03-06T07:51:16.098194Z"
    },
    "papermill": {
     "duration": 0.269669,
     "end_time": "2021-03-06T07:51:16.098878",
     "exception": false,
     "start_time": "2021-03-06T07:51:15.829209",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('/home/yuki/RANZCR/input/ranzcr-clip-catheter-line-classification/train.csv')\n",
    "test = pd.read_csv('/home/yuki/RANZCR/input/ranzcr-clip-catheter-line-classification/sample_submission.csv')\n",
    "train_annotations = pd.read_csv('/home/yuki/RANZCR/input/ranzcr-clip-catheter-line-classification/train_annotations.csv')\n",
    "\n",
    "# delete suspicious data\n",
    "train = train[train['StudyInstanceUID'] != '1.2.826.0.1.3680043.8.498.93345761486297843389996628528592497280'].reset_index(drop=True)\n",
    "train_annotations = train_annotations[train_annotations['StudyInstanceUID'] != '1.2.826.0.1.3680043.8.498.93345761486297843389996628528592497280'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.015561,
     "end_time": "2021-03-06T07:51:16.130403",
     "exception": false,
     "start_time": "2021-03-06T07:51:16.114842",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## CFG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-06T07:51:16.169757Z",
     "iopub.status.busy": "2021-03-06T07:51:16.169022Z",
     "iopub.status.idle": "2021-03-06T07:51:16.171892Z",
     "shell.execute_reply": "2021-03-06T07:51:16.171497Z"
    },
    "papermill": {
     "duration": 0.025733,
     "end_time": "2021-03-06T07:51:16.172025",
     "exception": false,
     "start_time": "2021-03-06T07:51:16.146292",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# CFG\n",
    "# ====================================================\n",
    "class CFG:\n",
    "    debug=False\n",
    "    device='GPU' # ['TPU', 'GPU']\n",
    "    nprocs=1 # [1, 8]\n",
    "    print_freq=100\n",
    "    num_workers=4\n",
    "    model_name='tf_efficientnet_b5_ns'\n",
    "    student='/home/yuki/RANZCR/input/efficientnetb5cv9621/tf_efficientnet_b5_ns_CV96.21.pth'\n",
    "    teacher='/home/yuki/RANZCR/input/015-training-efficientnetb5ns-step1-data/tf_efficientnet_b5_ns_fold1_best_loss.pth'\n",
    "    weights=[0.5, 1]\n",
    "    size=640\n",
    "    scheduler='CosineAnnealingLR' # ['ReduceLROnPlateau', 'CosineAnnealingLR', 'CosineAnnealingWarmRestarts']\n",
    "    epochs=5\n",
    "    #factor=0.2 # ReduceLROnPlateau\n",
    "    #patience=4 # ReduceLROnPlateau\n",
    "    #eps=1e-6 # ReduceLROnPlateau\n",
    "    T_max=5 # CosineAnnealingLR\n",
    "    #T_0=5 # CosineAnnealingWarmRestarts\n",
    "    lr=1e-5 # 1e-4\n",
    "    min_lr=1e-6\n",
    "    batch_size=8 # 64\n",
    "    weight_decay=1e-6\n",
    "    gradient_accumulation_steps=1\n",
    "    max_grad_norm=1000\n",
    "    seed=416\n",
    "    target_size=11\n",
    "    target_cols=['ETT - Abnormal', 'ETT - Borderline', 'ETT - Normal',\n",
    "                 'NGT - Abnormal', 'NGT - Borderline', 'NGT - Incompletely Imaged', 'NGT - Normal', \n",
    "                 'CVC - Abnormal', 'CVC - Borderline', 'CVC - Normal',\n",
    "                 'Swan Ganz Catheter Present']\n",
    "    n_fold=5\n",
    "    trn_fold=[0] # [0, 1, 2, 3, 4]\n",
    "    train=True\n",
    "    \n",
    "if CFG.debug:\n",
    "    CFG.epochs = 3\n",
    "    train = train.sample(n=3000, random_state=CFG.seed).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-06T07:51:16.20992Z",
     "iopub.status.busy": "2021-03-06T07:51:16.209257Z",
     "iopub.status.idle": "2021-03-06T07:51:16.212138Z",
     "shell.execute_reply": "2021-03-06T07:51:16.211621Z"
    },
    "papermill": {
     "duration": 0.023325,
     "end_time": "2021-03-06T07:51:16.212246",
     "exception": false,
     "start_time": "2021-03-06T07:51:16.188921",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if CFG.device == 'TPU':\n",
    "    import os\n",
    "    os.system('curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py')\n",
    "    os.system('python pytorch-xla-env-setup.py --version 1.7 --apt-packages libomp5 libopenblas-dev')\n",
    "    os.system('export XLA_USE_BF16=1')\n",
    "    import torch_xla.core.xla_model as xm\n",
    "    import torch_xla.distributed.parallel_loader as pl\n",
    "    import torch_xla.distributed.xla_multiprocessing as xmp\n",
    "    CFG.lr = CFG.lr * CFG.nprocs\n",
    "    CFG.batch_size = CFG.batch_size // CFG.nprocs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.01567,
     "end_time": "2021-03-06T07:51:16.243748",
     "exception": false,
     "start_time": "2021-03-06T07:51:16.228078",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-06T07:51:16.285602Z",
     "iopub.status.busy": "2021-03-06T07:51:16.284944Z",
     "iopub.status.idle": "2021-03-06T07:51:20.414088Z",
     "shell.execute_reply": "2021-03-06T07:51:20.413457Z"
    },
    "papermill": {
     "duration": 4.154532,
     "end_time": "2021-03-06T07:51:20.414217",
     "exception": false,
     "start_time": "2021-03-06T07:51:16.259685",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Library\n",
    "# ====================================================\n",
    "import sys\n",
    "sys.path.append('/home/yuki/RANZCR/input/pytorch-images-seresnet')\n",
    "\n",
    "import os\n",
    "import ast\n",
    "import copy\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from contextlib import contextmanager\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from functools import partial\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, SGD\n",
    "import torchvision.models as models\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CosineAnnealingLR, ReduceLROnPlateau\n",
    "\n",
    "from albumentations import (\n",
    "    Compose, OneOf, Normalize, Resize, RandomResizedCrop, RandomCrop, HorizontalFlip, VerticalFlip, \n",
    "    RandomBrightness, RandomContrast, RandomBrightnessContrast, Rotate, ShiftScaleRotate, Cutout, \n",
    "    IAAAdditiveGaussianNoise, Transpose, HueSaturationValue, CoarseDropout\n",
    "    )\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from albumentations import ImageOnlyTransform\n",
    "\n",
    "import timm\n",
    "\n",
    "if CFG.device == 'TPU':\n",
    "    import ignite.distributed as idist\n",
    "elif CFG.device == 'GPU':\n",
    "    from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.016957,
     "end_time": "2021-03-06T07:51:20.449117",
     "exception": false,
     "start_time": "2021-03-06T07:51:20.43216",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-06T07:51:20.491995Z",
     "iopub.status.busy": "2021-03-06T07:51:20.491346Z",
     "iopub.status.idle": "2021-03-06T07:51:20.498525Z",
     "shell.execute_reply": "2021-03-06T07:51:20.497938Z"
    },
    "papermill": {
     "duration": 0.032823,
     "end_time": "2021-03-06T07:51:20.498642",
     "exception": false,
     "start_time": "2021-03-06T07:51:20.465819",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Utils\n",
    "# ====================================================\n",
    "def get_score(y_true, y_pred):\n",
    "    scores = []\n",
    "    for i in range(y_true.shape[1]):\n",
    "        score = roc_auc_score(y_true[:,i], y_pred[:,i])\n",
    "        scores.append(score)\n",
    "    avg_score = np.mean(scores)\n",
    "    return avg_score, scores\n",
    "\n",
    "@contextmanager\n",
    "def timer(name):\n",
    "    t0 = time.time()\n",
    "    LOGGER.info(f'[{name}] start')\n",
    "    yield\n",
    "    LOGGER.info(f'[{name}] done in {time.time() - t0:.0f} s.')\n",
    "\n",
    "\n",
    "def init_logger(log_file=OUTPUT_DIR+'train.log'):\n",
    "    from logging import getLogger, INFO, FileHandler,  Formatter,  StreamHandler\n",
    "    logger = getLogger(__name__)\n",
    "    logger.setLevel(INFO)\n",
    "    handler1 = StreamHandler()\n",
    "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
    "    handler2 = FileHandler(filename=log_file)\n",
    "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    return logger\n",
    "\n",
    "LOGGER = init_logger()\n",
    "\n",
    "def seed_torch(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed_torch(seed=CFG.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.016328,
     "end_time": "2021-03-06T07:51:20.531439",
     "exception": false,
     "start_time": "2021-03-06T07:51:20.515111",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## CV splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-06T07:51:20.570288Z",
     "iopub.status.busy": "2021-03-06T07:51:20.569504Z",
     "iopub.status.idle": "2021-03-06T07:51:20.636087Z",
     "shell.execute_reply": "2021-03-06T07:51:20.636511Z"
    },
    "papermill": {
     "duration": 0.08887,
     "end_time": "2021-03-06T07:51:20.636662",
     "exception": false,
     "start_time": "2021-03-06T07:51:20.547792",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "folds = train.copy()\n",
    "Fold = GroupKFold(n_splits=CFG.n_fold)\n",
    "groups = folds['PatientID'].values\n",
    "for n, (train_index, val_index) in enumerate(Fold.split(folds, folds[CFG.target_cols], groups)):\n",
    "    folds.loc[val_index, 'fold'] = int(n)\n",
    "folds['fold'] = folds['fold'].astype(int)\n",
    "display(folds.groupby('fold').size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.016745,
     "end_time": "2021-03-06T07:51:20.670395",
     "exception": false,
     "start_time": "2021-03-06T07:51:20.65365",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-06T07:51:20.717689Z",
     "iopub.status.busy": "2021-03-06T07:51:20.716891Z",
     "iopub.status.idle": "2021-03-06T07:51:20.719691Z",
     "shell.execute_reply": "2021-03-06T07:51:20.719279Z"
    },
    "papermill": {
     "duration": 0.032592,
     "end_time": "2021-03-06T07:51:20.7198",
     "exception": false,
     "start_time": "2021-03-06T07:51:20.687208",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Dataset\n",
    "# ====================================================\n",
    "COLOR_MAP = {'ETT - Abnormal': (255, 0, 0),\n",
    "             'ETT - Borderline': (0, 255, 0),\n",
    "             'ETT - Normal': (0, 0, 255),\n",
    "             'NGT - Abnormal': (255, 255, 0),\n",
    "             'NGT - Borderline': (255, 0, 255),\n",
    "             'NGT - Incompletely Imaged': (0, 255, 255),\n",
    "             'NGT - Normal': (128, 0, 0),\n",
    "             'CVC - Abnormal': (0, 128, 0),\n",
    "             'CVC - Borderline': (0, 0, 128),\n",
    "             'CVC - Normal': (128, 128, 0),\n",
    "             'Swan Ganz Catheter Present': (128, 0, 128),\n",
    "            }\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, df, df_annotations, use_annot=False, annot_size=50, transform=None):\n",
    "        self.df = df\n",
    "        self.df_annotations = df_annotations\n",
    "        self.use_annot = use_annot\n",
    "        self.annot_size = annot_size\n",
    "        self.file_names = df['StudyInstanceUID'].values\n",
    "        self.labels = df[CFG.target_cols].values\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        file_name = self.file_names[idx]\n",
    "        file_path = f'{TRAIN_PATH}/{file_name}.jpg'\n",
    "        image = cv2.imread(file_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        labels = torch.tensor(self.labels[idx]).float()\n",
    "        if self.use_annot:\n",
    "            image_annot = image.copy()\n",
    "            query_string = f\"StudyInstanceUID == '{file_name}'\"\n",
    "            df = self.df_annotations.query(query_string)\n",
    "            for i, row in df.iterrows():\n",
    "                label = row[\"label\"]\n",
    "                data = np.array(ast.literal_eval(row[\"data\"]))\n",
    "                for d in data:\n",
    "                    image_annot[d[1]-self.annot_size//2:d[1]+self.annot_size//2,\n",
    "                                d[0]-self.annot_size//2:d[0]+self.annot_size//2,\n",
    "                                :] = COLOR_MAP[label]\n",
    "            if self.transform:\n",
    "                augmented = self.transform(image=image, image_annot=image_annot)\n",
    "                image = augmented['image']\n",
    "                image_annot = augmented['image_annot']\n",
    "            return image, image_annot, labels\n",
    "        \n",
    "        else:\n",
    "            if self.transform:\n",
    "                augmented = self.transform(image=image)\n",
    "                image = augmented['image']\n",
    "            return image, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.016833,
     "end_time": "2021-03-06T07:51:20.75332",
     "exception": false,
     "start_time": "2021-03-06T07:51:20.736487",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-06T07:51:20.801261Z",
     "iopub.status.busy": "2021-03-06T07:51:20.80047Z",
     "iopub.status.idle": "2021-03-06T07:51:20.803538Z",
     "shell.execute_reply": "2021-03-06T07:51:20.803114Z"
    },
    "papermill": {
     "duration": 0.031897,
     "end_time": "2021-03-06T07:51:20.803654",
     "exception": false,
     "start_time": "2021-03-06T07:51:20.771757",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Transforms\n",
    "# ====================================================\n",
    "def get_transforms(*, data):\n",
    "    \n",
    "    if data == 'train':\n",
    "        return Compose([\n",
    "            #Resize(CFG.size, CFG.size),\n",
    "            RandomResizedCrop(CFG.size, CFG.size, scale=(0.85, 1.0)),\n",
    "            HorizontalFlip(p=0.5),\n",
    "            RandomBrightnessContrast(p=0.2, brightness_limit=(-0.2, 0.2), contrast_limit=(-0.2, 0.2)),\n",
    "            HueSaturationValue(p=0.2, hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2),\n",
    "            ShiftScaleRotate(p=0.2, shift_limit=0.0625, scale_limit=0.2, rotate_limit=20),\n",
    "            CoarseDropout(p=0.2),\n",
    "            Cutout(p=0.2, max_h_size=16, max_w_size=16, fill_value=(0., 0., 0.), num_holes=16),\n",
    "            Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225],\n",
    "            ),\n",
    "            ToTensorV2(),\n",
    "        ], additional_targets={'image_annot': 'image'})\n",
    "    \n",
    "    elif data == 'check':\n",
    "        return Compose([\n",
    "            #Resize(CFG.size, CFG.size),\n",
    "            RandomResizedCrop(CFG.size, CFG.size, scale=(0.85, 1.0)),\n",
    "            HorizontalFlip(p=0.5),\n",
    "            RandomBrightnessContrast(p=0.2, brightness_limit=(-0.2, 0.2), contrast_limit=(-0.2, 0.2)),\n",
    "            HueSaturationValue(p=0.2, hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2),\n",
    "            ShiftScaleRotate(p=0.2, shift_limit=0.0625, scale_limit=0.2, rotate_limit=20),\n",
    "            CoarseDropout(p=0.2),\n",
    "            Cutout(p=0.2, max_h_size=16, max_w_size=16, fill_value=(0., 0., 0.), num_holes=16),\n",
    "            #Normalize(\n",
    "            #    mean=[0.485, 0.456, 0.406],\n",
    "            #    std=[0.229, 0.224, 0.225],\n",
    "            #),\n",
    "            ToTensorV2(),\n",
    "        ], additional_targets={'image_annot': 'image'})\n",
    "    \n",
    "    elif data == 'valid':\n",
    "        return Compose([\n",
    "            Resize(CFG.size, CFG.size),\n",
    "            Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225],\n",
    "            ),\n",
    "            ToTensorV2(),\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-06T07:51:20.846265Z",
     "iopub.status.busy": "2021-03-06T07:51:20.84554Z",
     "iopub.status.idle": "2021-03-06T07:51:23.339093Z",
     "shell.execute_reply": "2021-03-06T07:51:23.339508Z"
    },
    "papermill": {
     "duration": 2.518978,
     "end_time": "2021-03-06T07:51:23.339653",
     "exception": false,
     "start_time": "2021-03-06T07:51:20.820675",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "train_dataset = TrainDataset(folds[folds['StudyInstanceUID'].isin(train_annotations['StudyInstanceUID'].unique())].reset_index(drop=True), \n",
    "                             train_annotations, use_annot=True, transform=get_transforms(data='check'))\n",
    "\n",
    "for i in range(5):\n",
    "    image, image_annot, label = train_dataset[i]\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(8, 8))\n",
    "    axes[0].imshow(image.transpose(0, 1).transpose(1, 2))  # どのタイミングでshapeが(C, H, W)になっていたのかがわからない。\n",
    "    axes[1].imshow(image_annot.transpose(0, 1).transpose(1, 2))\n",
    "    plt.title(f'label: {label}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.024533,
     "end_time": "2021-03-06T07:51:23.389875",
     "exception": false,
     "start_time": "2021-03-06T07:51:23.365342",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-06T07:51:23.460943Z",
     "iopub.status.busy": "2021-03-06T07:51:23.459358Z",
     "iopub.status.idle": "2021-03-06T07:51:23.462319Z",
     "shell.execute_reply": "2021-03-06T07:51:23.461882Z"
    },
    "papermill": {
     "duration": 0.04765,
     "end_time": "2021-03-06T07:51:23.462437",
     "exception": false,
     "start_time": "2021-03-06T07:51:23.414787",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PAM_Module(nn.Module):\n",
    "    \"\"\" Position attention module\"\"\"\n",
    "    #Ref from SAGAN\n",
    "    def __init__(self, in_dim):\n",
    "        super(PAM_Module, self).__init__()\n",
    "        self.chanel_in = in_dim\n",
    "\n",
    "        self.query_conv = nn.Conv2d(in_channels=in_dim, out_channels=in_dim//8, kernel_size=1)\n",
    "        self.key_conv = nn.Conv2d(in_channels=in_dim, out_channels=in_dim//8, kernel_size=1)\n",
    "        self.value_conv = nn.Conv2d(in_channels=in_dim, out_channels=in_dim, kernel_size=1)\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "            inputs :\n",
    "                x : input feature maps( B X C X H X W)\n",
    "            returns :\n",
    "                out : attention value + input feature\n",
    "                attention: B X (HxW) X (HxW)\n",
    "        \"\"\"\n",
    "        m_batchsize, C, height, width = x.size()\n",
    "        proj_query = self.query_conv(x).view(m_batchsize, -1, width*height).permute(0, 2, 1)\n",
    "        proj_key = self.key_conv(x).view(m_batchsize, -1, width*height)\n",
    "        energy = torch.bmm(proj_query, proj_key)\n",
    "        attention = torch.softmax(energy, dim=-1)\n",
    "        proj_value = self.value_conv(x).view(m_batchsize, -1, width*height)\n",
    "\n",
    "        out = torch.bmm(proj_value, attention.permute(0, 2, 1))\n",
    "        out = out.view(m_batchsize, C, height, width)\n",
    "\n",
    "        out = self.gamma*out + x\n",
    "        return out\n",
    "\n",
    "\n",
    "class CAM_Module(nn.Module):\n",
    "    \"\"\" Channel attention module\"\"\"\n",
    "    def __init__(self, in_dim):\n",
    "        super(CAM_Module, self).__init__()\n",
    "        self.chanel_in = in_dim\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "            inputs :\n",
    "                x : input feature maps( B X C X H X W)\n",
    "            returns :\n",
    "                out : attention value + input feature\n",
    "                attention: B X C X C\n",
    "        \"\"\"\n",
    "        m_batchsize, C, height, width = x.size()\n",
    "        proj_query = x.view(m_batchsize, C, -1)\n",
    "        proj_key = x.view(m_batchsize, C, -1).permute(0, 2, 1)\n",
    "        energy = torch.bmm(proj_query, proj_key)\n",
    "        energy_new = torch.max(energy, -1, keepdim=True)[0].expand_as(energy)-energy\n",
    "        attention = torch.softmax(energy_new, dim=-1)\n",
    "        proj_value = x.view(m_batchsize, C, -1)\n",
    "\n",
    "        out = torch.bmm(attention, proj_value)\n",
    "        out = out.view(m_batchsize, C, height, width)\n",
    "\n",
    "        out = self.gamma*out + x\n",
    "        return out\n",
    "\n",
    "\n",
    "class CBAM(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        # def __init__(self):\n",
    "        super(CBAM, self).__init__()\n",
    "        inter_channels = in_channels // 4\n",
    "        self.conv1_c = nn.Sequential(nn.Conv2d(in_channels, inter_channels, 3, padding=1, bias=False),\n",
    "                                     nn.BatchNorm2d(inter_channels),\n",
    "                                     nn.ReLU(inplace=True))\n",
    "        \n",
    "        self.conv1_s = nn.Sequential(nn.Conv2d(in_channels, inter_channels, 3, padding=1, bias=False),\n",
    "                                     nn.BatchNorm2d(inter_channels),\n",
    "                                     nn.ReLU(inplace=True))\n",
    "\n",
    "        self.channel_gate = CAM_Module(inter_channels)\n",
    "        self.spatial_gate = PAM_Module(inter_channels)\n",
    "\n",
    "        self.conv2_c = nn.Sequential(nn.Conv2d(inter_channels, in_channels, 3, padding=1, bias=False),\n",
    "                                     nn.BatchNorm2d(in_channels),\n",
    "                                     nn.ReLU(inplace=True))\n",
    "        self.conv2_a = nn.Sequential(nn.Conv2d(inter_channels, in_channels, 3, padding=1, bias=False),\n",
    "                                     nn.BatchNorm2d(in_channels),\n",
    "                                     nn.ReLU(inplace=True))\n",
    "\n",
    "    def forward(self, x):\n",
    "        feat1 = self.conv1_c(x)\n",
    "        chnl_att = self.channel_gate(feat1)\n",
    "        chnl_att = self.conv2_c(chnl_att)\n",
    "\n",
    "        feat2 = self.conv1_s(x)\n",
    "        spat_att = self.spatial_gate(feat2)\n",
    "        spat_att = self.conv2_a(spat_att)\n",
    "\n",
    "        x_out = chnl_att + spat_att\n",
    "\n",
    "        return x_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-06T07:51:23.530106Z",
     "iopub.status.busy": "2021-03-06T07:51:23.528413Z",
     "iopub.status.idle": "2021-03-06T07:51:23.530797Z",
     "shell.execute_reply": "2021-03-06T07:51:23.531234Z"
    },
    "papermill": {
     "duration": 0.044278,
     "end_time": "2021-03-06T07:51:23.531378",
     "exception": false,
     "start_time": "2021-03-06T07:51:23.4871",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# MODEL\n",
    "# ====================================================\n",
    "class CustomEfficientNetB5ns(nn.Module):\n",
    "    def __init__(self, model_name='tf_efficientnet_b5_ns', pretrained=False):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model(model_name, pretrained=False)\n",
    "        n_features  = self.model.classifier.in_features\n",
    "        self.model.classifier = nn.Linear(n_features, CFG.target_size)\n",
    "        if pretrained:\n",
    "            pretrained_path = '/home/yuki/RANZCR/input/efficientnetb5cv9621/tf_efficientnet_b5_ns_CV96.21.pth'            \n",
    "            checkpoint = torch.load(pretrained_path, map_location='cpu')['model']\n",
    "            for key in list(checkpoint.keys()):\n",
    "                if 'model.' in key:\n",
    "                    checkpoint[key.replace('model.', '')] = checkpoint[key]\n",
    "                    del checkpoint[key]\n",
    "            self.model.load_state_dict(checkpoint) \n",
    "            \n",
    "            print(f'load {model_name} pretrained model')\n",
    "        n_features = self.model.classifier.in_features\n",
    "        self.model.global_pool = nn.Identity()\n",
    "        self.model.classifier = nn.Identity()\n",
    "        self.pooling = nn.AdaptiveAvgPool2d(1)\n",
    "        self.classifier = nn.Linear(n_features, CFG.target_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        bs = x.size(0)\n",
    "        features = self.model(x)\n",
    "        pooled_features = self.pooling(features).view(bs, -1)\n",
    "        output = self.classifier(pooled_features)\n",
    "        return features, pooled_features, output\n",
    "        \n",
    "    \n",
    "\n",
    "class CustomEfficientNetB5ns_WLF(nn.Module):\n",
    "    def __init__(self, model_name='tf_efficientnet_b5_ns', pretrained=False):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model(model_name, pretrained=False)\n",
    "        n_features  = self.model.classifier.in_features\n",
    "        self.model.classifier = nn.Linear(n_features, CFG.target_size)\n",
    "        if pretrained:\n",
    "            pretrained_path = '/home/yuki/RANZCR/input/efficientnetb5cv9621/tf_efficientnet_b5_ns_CV96.21.pth'\n",
    "            checkpoint = torch.load(pretrained_path, map_location='cpu')['model']\n",
    "            for key in list(checkpoint.keys()):\n",
    "                if 'model.' in key:\n",
    "                    checkpoint[key.replace('model.', '')] = checkpoint[key]\n",
    "                    del checkpoint[key]\n",
    "            self.model.load_state_dict(checkpoint) \n",
    "            \n",
    "            print(f'load {model_name} pretrained model')\n",
    "        n_features = self.model.classifier.in_features\n",
    "        self.model.global_pool = nn.Identity()\n",
    "        self.model.classifier = nn.Identity()\n",
    "        self.pooling = nn.AdaptiveAvgPool2d(1)\n",
    "#         self.fc = nn.Linear(n_features, CFG.target_size)\n",
    "        \n",
    "        self.local_fe = CBAM(n_features)\n",
    "        self.classifier = nn.Sequential(nn.Linear(n_features + n_features, n_features), \n",
    "                                        nn.BatchNorm1d(n_features),\n",
    "                                        nn.ReLU(inplace=True),\n",
    "                                        nn.Linear(n_features, CFG.target_size))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        bs = x.size(0)\n",
    "        features = self.model(x)\n",
    "        \n",
    "        global_features = self.pooling(features).view(bs, -1)\n",
    "        \n",
    "        local_features = self.local_fe(features)\n",
    "        local_features = torch.sum(local_features, dim=[2, 3])  # ここ、どうしてsum？GAPではない？\n",
    "        \n",
    "        all_features = torch.cat([global_features, local_features], dim=1)\n",
    "        output = self.classifier(all_features)\n",
    "        \n",
    "        return features, all_features, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-06T07:51:23.587305Z",
     "iopub.status.busy": "2021-03-06T07:51:23.586652Z",
     "iopub.status.idle": "2021-03-06T07:51:23.590439Z",
     "shell.execute_reply": "2021-03-06T07:51:23.590009Z"
    },
    "papermill": {
     "duration": 0.034147,
     "end_time": "2021-03-06T07:51:23.590546",
     "exception": false,
     "start_time": "2021-03-06T07:51:23.556399",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self, weights=[1, 1]):\n",
    "        super(CustomLoss, self).__init__()\n",
    "        self.weights = weights\n",
    "        \n",
    "    def forward(self, teacher_features, features, y_pred, labels):\n",
    "        consistency_loss = nn.MSELoss()(teacher_features.view(-1), features.view(-1))\n",
    "        cls_loss = nn.BCEWithLogitsLoss()(y_pred, labels)\n",
    "        loss = self.weights[0] * consistency_loss + self.weights[1] * cls_loss\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.02516,
     "end_time": "2021-03-06T07:51:23.640681",
     "exception": false,
     "start_time": "2021-03-06T07:51:23.615521",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## LOSS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.024602,
     "end_time": "2021-03-06T07:51:23.69066",
     "exception": false,
     "start_time": "2021-03-06T07:51:23.666058",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-06T07:51:23.772294Z",
     "iopub.status.busy": "2021-03-06T07:51:23.770407Z",
     "iopub.status.idle": "2021-03-06T07:51:23.772903Z",
     "shell.execute_reply": "2021-03-06T07:51:23.77332Z"
    },
    "papermill": {
     "duration": 0.057555,
     "end_time": "2021-03-06T07:51:23.773469",
     "exception": false,
     "start_time": "2021-03-06T07:51:23.715914",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Helper functions\n",
    "# ====================================================\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "        \n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "def train_fn(train_loader, teacher_model, model, criterion, optimizer, epoch, scheduler, device):\n",
    "    if CFG.device == 'GPU':\n",
    "        scaler = GradScaler()\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    scores = AverageMeter()\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "    start = end = time.time()\n",
    "    global_step = 0\n",
    "    for step, (images, images_annot, labels) in enumerate(train_loader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "        with torch.no_grad():\n",
    "            teacher_features, _, _ = teacher_model(images_annot.to(device))\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = labels.size(0)\n",
    "        if CFG.device == 'GPU':\n",
    "            with autocast():\n",
    "                features, _, y_preds = model(images)\n",
    "                loss = criterion(teacher_features, features, y_preds, labels)\n",
    "                # record loss\n",
    "                losses.update(loss.item(), batch_size)\n",
    "                if CFG.gradient_accumulation_steps > 1:\n",
    "                    loss = loss / CFG.gradient_accumulation_steps\n",
    "                scaler.scale(loss).backward()\n",
    "                grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n",
    "                if (step + 1) % CFG.gradient_accumulation_steps == 0:\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                    optimizer.zero_grad()\n",
    "                    global_step += 1\n",
    "        elif CFG.device == 'TPU':\n",
    "            features, _, y_preds = model(images)\n",
    "            loss = criterion(teacher_features, features, y_preds, labels)\n",
    "            # record loss\n",
    "            losses.update(loss.item(), batch_size)\n",
    "            if CFG.gradient_accumulation_steps > 1:\n",
    "                loss = loss / CFG.gradient_accumulation_steps\n",
    "            loss.backward()\n",
    "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n",
    "            if (step + 1) % CFG.gradient_accumulation_steps == 0:\n",
    "                xm.optimizer_step(optimizer, barrier=True)\n",
    "                optimizer.zero_grad()\n",
    "                global_step += 1\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time() \n",
    "        if CFG.device == 'GPU':\n",
    "            if step % CFG.print_freq == 0 or step == (len(train_loader)-1):\n",
    "                print('Epoch: [{0}][{1}/{2}] '\n",
    "                      'Data {data_time.val:.3f} ({data_time.avg:.3f}) '\n",
    "                      'Elapsed {remain:s} '\n",
    "                      'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                      'Grad: {grad_norm:.4f}  '\n",
    "                      #'LR: {lr:.6f}  '\n",
    "                      .format(\n",
    "                       epoch+1, step, len(train_loader), batch_time=batch_time,\n",
    "                       data_time=data_time, loss=losses,\n",
    "                       remain=timeSince(start, float(step+1)/len(train_loader)),\n",
    "                       grad_norm=grad_norm,\n",
    "                       #lr=scheduler.get_lr()[0],\n",
    "                       ))\n",
    "        elif CFG.device == 'TPU':\n",
    "            if step % CFG.print_freq == 0 or step == (len(train_loader)-1):\n",
    "                xm.master_print('Epoch: [{0}][{1}/{2}] '\n",
    "                                'Data {data_time.val:.3f} ({data_time.avg:.3f}) '\n",
    "                                'Elapsed {remain:s} '\n",
    "                                'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                                'Grad: {grad_norm:.4f}  '\n",
    "                                #'LR: {lr:.6f}  '\n",
    "                                .format(\n",
    "                                epoch+1, step, len(train_loader), batch_time=batch_time,\n",
    "                                data_time=data_time, loss=losses,\n",
    "                                remain=timeSince(start, float(step+1)/len(train_loader)),\n",
    "                                grad_norm=grad_norm,\n",
    "                                #lr=scheduler.get_lr()[0],\n",
    "                                ))\n",
    "    return losses.avg\n",
    "\n",
    "def valid_fn(valid_loader, model, criterion, device):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    scores = AverageMeter()\n",
    "    # switch to evaluation mode\n",
    "    model.eval()\n",
    "    trues = []\n",
    "    preds = []\n",
    "    start = end = time.time()\n",
    "    for step, (images, labels) in enumerate(valid_loader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = labels.size(0)\n",
    "        # compute loss\n",
    "        with torch.no_grad():\n",
    "            _, _, y_preds = model(images)\n",
    "        loss = criterion(y_preds, labels)\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        # record accuracy\n",
    "        trues.append(labels.to('cpu').numpy())\n",
    "        preds.append(y_preds.sigmoid().to('cpu').numpy())\n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        if CFG.device == 'GPU':\n",
    "            if step % CFG.print_freq == 0 or step == (len(valid_loader)-1):\n",
    "                print('EVAL: [{0}/{1}] '\n",
    "                      'Data {data_time.val:.3f} ({data_time.avg:.3f}) '\n",
    "                      'Elapsed {remain:s} '\n",
    "                      'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                      .format(\n",
    "                       step, len(valid_loader), batch_time=batch_time,\n",
    "                       data_time=data_time, loss=losses,\n",
    "                       remain=timeSince(start, float(step+1)/len(valid_loader)),\n",
    "                       ))\n",
    "        elif CFG.device == 'TPU':\n",
    "            if step % CFG.print_freq == 0 or step == (len(valid_loader)-1):\n",
    "                xm.master_print('EVAL: [{0}/{1}] '\n",
    "                                'Data {data_time.val:.3f} ({data_time.avg:.3f}) '\n",
    "                                'Elapsed {remain:s} '\n",
    "                                'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                                .format(\n",
    "                                step, len(valid_loader), batch_time=batch_time,\n",
    "                                data_time=data_time, loss=losses,\n",
    "                                remain=timeSince(start, float(step+1)/len(valid_loader)),\n",
    "                                ))\n",
    "    trues = np.concatenate(trues)\n",
    "    predictions = np.concatenate(preds)\n",
    "    return losses.avg, predictions, trues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.025232,
     "end_time": "2021-03-06T07:51:23.823887",
     "exception": false,
     "start_time": "2021-03-06T07:51:23.798655",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-06T07:51:23.909422Z",
     "iopub.status.busy": "2021-03-06T07:51:23.882879Z",
     "iopub.status.idle": "2021-03-06T07:51:23.9204Z",
     "shell.execute_reply": "2021-03-06T07:51:23.91989Z"
    },
    "papermill": {
     "duration": 0.071171,
     "end_time": "2021-03-06T07:51:23.920524",
     "exception": false,
     "start_time": "2021-03-06T07:51:23.849353",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Train loop\n",
    "# ====================================================\n",
    "def train_loop(folds, fold):\n",
    "\n",
    "    if CFG.device == 'GPU':\n",
    "        LOGGER.info(f\"========== fold: {fold} training ==========\")\n",
    "    elif CFG.device == 'TPU':\n",
    "        if CFG.nprocs == 1:\n",
    "            LOGGER.info(f\"========== fold: {fold} training ==========\")\n",
    "        elif CFG.nprocs == 8:\n",
    "            xm.master_print(f\"========== fold: {fold} training ==========\")\n",
    "            \n",
    "    # ====================================================\n",
    "    # loader\n",
    "    # ====================================================\n",
    "    trn_idx = folds[folds['fold'] != fold].index\n",
    "    val_idx = folds[folds['fold'] == fold].index\n",
    "\n",
    "    train_folds = folds.loc[trn_idx].reset_index(drop=True)\n",
    "    valid_folds = folds.loc[val_idx].reset_index(drop=True)\n",
    "    \n",
    "    train_folds = train_folds[train_folds['StudyInstanceUID'].isin(train_annotations['StudyInstanceUID'].unique())].reset_index(drop=True)\n",
    "    \n",
    "    valid_labels = valid_folds[CFG.target_cols].values\n",
    "    \n",
    "    train_dataset = TrainDataset(train_folds, train_annotations, use_annot=True,\n",
    "                                 transform=get_transforms(data='train'))\n",
    "    valid_dataset = TrainDataset(valid_folds, train_annotations, use_annot=False,\n",
    "                                 transform=get_transforms(data='valid'))\n",
    "    \n",
    "    if CFG.device == 'GPU':\n",
    "        train_loader = DataLoader(train_dataset, \n",
    "                                  batch_size=CFG.batch_size, \n",
    "                                  shuffle=True, \n",
    "                                  num_workers=CFG.num_workers, pin_memory=True, drop_last=True)\n",
    "        valid_loader = DataLoader(valid_dataset, \n",
    "                                  batch_size=CFG.batch_size * 2, \n",
    "                                  shuffle=False, \n",
    "                                  num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n",
    "        \n",
    "    elif CFG.device == 'TPU':\n",
    "        train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset,\n",
    "                                                                        num_replicas=xm.xrt_world_size(),\n",
    "                                                                        rank=xm.get_ordinal(),\n",
    "                                                                        shuffle=True)\n",
    "        train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                                   batch_size=CFG.batch_size,\n",
    "                                                   sampler=train_sampler,\n",
    "                                                   drop_last=True,\n",
    "                                                   num_workers=CFG.num_workers)\n",
    "        \n",
    "        valid_sampler = torch.utils.data.distributed.DistributedSampler(valid_dataset,\n",
    "                                                                        num_replicas=xm.xrt_world_size(),\n",
    "                                                                        rank=xm.get_ordinal(),\n",
    "                                                                        shuffle=False)\n",
    "        valid_loader = torch.utils.data.DataLoader(valid_dataset,\n",
    "                                                   batch_size=CFG.batch_size * 2,\n",
    "                                                   sampler=valid_sampler,\n",
    "                                                   drop_last=False,\n",
    "                                                   num_workers=CFG.num_workers)\n",
    "        \n",
    "    # ====================================================\n",
    "    # scheduler \n",
    "    # ====================================================\n",
    "    def get_scheduler(optimizer):\n",
    "        if CFG.scheduler=='ReduceLROnPlateau':\n",
    "            scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=CFG.factor, patience=CFG.patience, verbose=True, eps=CFG.eps)\n",
    "        elif CFG.scheduler=='CosineAnnealingLR':\n",
    "            scheduler = CosineAnnealingLR(optimizer, T_max=CFG.T_max, eta_min=CFG.min_lr, last_epoch=-1)\n",
    "        elif CFG.scheduler=='CosineAnnealingWarmRestarts':\n",
    "            scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=CFG.T_0, T_mult=1, eta_min=CFG.min_lr, last_epoch=-1)\n",
    "        return scheduler\n",
    "    \n",
    "    # ====================================================\n",
    "    # model & optimizer\n",
    "    # ====================================================\n",
    "    if CFG.device == 'TPU':\n",
    "        device = xm.xla_device()\n",
    "    elif CFG.device == 'GPU':\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "    teacher_model = CustomEfficientNetB5ns(CFG.model_name, pretrained=False)\n",
    "    teacher_model.to(device)\n",
    "    state = torch.load(CFG.teacher)\n",
    "    teacher_model.load_state_dict(state['model'])\n",
    "    for param in teacher_model.parameters():\n",
    "        param.requires_grad = False\n",
    "    teacher_model.eval()\n",
    "#     teacher_model.to(device)\n",
    "    \n",
    "    model = CustomEfficientNetB5ns_WLF(CFG.model_name, pretrained=True)\n",
    "    model.to(device)\n",
    "#     state = torch.load(CFG.student)\n",
    "#     model.load_state_dict(state['model'])\n",
    "\n",
    "    optimizer = Adam(model.parameters(), lr=CFG.lr, weight_decay=CFG.weight_decay, amsgrad=False)\n",
    "    scheduler = get_scheduler(optimizer)\n",
    "    \n",
    "    # ====================================================\n",
    "    # loop\n",
    "    # ====================================================\n",
    "    train_criterion = CustomLoss(weights=CFG.weights)\n",
    "    valid_criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    best_score = 0.\n",
    "    best_loss = np.inf\n",
    "    \n",
    "    for epoch in range(CFG.epochs):\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # train\n",
    "        if CFG.device == 'TPU':\n",
    "            if CFG.nprocs == 1:\n",
    "                avg_loss = train_fn(train_loader, teacher_model, model, train_criterion, optimizer, epoch, scheduler, device)\n",
    "            elif CFG.nprocs == 8:\n",
    "                para_train_loader = pl.ParallelLoader(train_loader, [device])\n",
    "                avg_loss = train_fn(para_train_loader.per_device_loader(device), teacher_model, model, train_criterion, optimizer, epoch, scheduler, device)\n",
    "        elif CFG.device == 'GPU':\n",
    "            avg_loss = train_fn(train_loader, teacher_model, model, train_criterion, optimizer, epoch, scheduler, device)\n",
    "        \n",
    "        # eval\n",
    "        if CFG.device == 'TPU':\n",
    "            if CFG.nprocs == 1:\n",
    "                avg_val_loss, preds, _ = valid_fn(valid_loader, model, valid_criterion, device)\n",
    "            elif CFG.nprocs == 8:\n",
    "                para_valid_loader = pl.ParallelLoader(valid_loader, [device])\n",
    "                avg_val_loss, preds, valid_labels = valid_fn(para_valid_loader.per_device_loader(device), model, valid_criterion, device)\n",
    "                preds = idist.all_gather(torch.tensor(preds)).to('cpu').numpy()\n",
    "                valid_labels = idist.all_gather(torch.tensor(valid_labels)).to('cpu').numpy()\n",
    "        elif CFG.device == 'GPU':\n",
    "            avg_val_loss, preds, _ = valid_fn(valid_loader, model, valid_criterion, device)\n",
    "            \n",
    "        if isinstance(scheduler, ReduceLROnPlateau):\n",
    "            scheduler.step(avg_val_loss)\n",
    "        elif isinstance(scheduler, CosineAnnealingLR):\n",
    "            scheduler.step()\n",
    "        elif isinstance(scheduler, CosineAnnealingWarmRestarts):\n",
    "            scheduler.step()\n",
    "            \n",
    "        # scoring\n",
    "        score, scores = get_score(valid_labels, preds)\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "        \n",
    "        if CFG.device == 'GPU':\n",
    "            LOGGER.info(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n",
    "            LOGGER.info(f'Epoch {epoch+1} - Score: {score:.4f}  Scores: {np.round(scores, decimals=4)}')\n",
    "        elif CFG.device == 'TPU':\n",
    "            if CFG.nprocs == 1:\n",
    "                LOGGER.info(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n",
    "                LOGGER.info(f'Epoch {epoch+1} - Score: {score:.4f}  Scores: {np.round(scores, decimals=4)}')\n",
    "            elif CFG.nprocs == 8:\n",
    "                xm.master_print(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n",
    "                xm.master_print(f'Epoch {epoch+1} - Score: {score:.4f}  Scores: {np.round(scores, decimals=4)}')\n",
    "                \n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            if CFG.device == 'GPU':\n",
    "                LOGGER.info(f'Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model')\n",
    "                torch.save({'model': model.state_dict(), \n",
    "                            'preds': preds},\n",
    "                           OUTPUT_DIR+f'{CFG.model_name}_fold{fold}_best_score.pth')\n",
    "            elif CFG.device == 'TPU':\n",
    "                if CFG.nprocs == 1:\n",
    "                    LOGGER.info(f'Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model')\n",
    "                elif CFG.nprocs == 8:\n",
    "                    xm.master_print(f'Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model')\n",
    "                xm.save({'model': model, \n",
    "                         'preds': preds}, \n",
    "                        OUTPUT_DIR+f'{CFG.model_name}_fold{fold}_best_score.pth')\n",
    "                \n",
    "        if avg_val_loss < best_loss:\n",
    "            best_loss = avg_val_loss\n",
    "            if CFG.device == 'GPU':\n",
    "                LOGGER.info(f'Epoch {epoch+1} - Save Best Loss: {best_loss:.4f} Model')\n",
    "                torch.save({'model': model.state_dict(), \n",
    "                            'preds': preds},\n",
    "                           OUTPUT_DIR+f'{CFG.model_name}_fold{fold}_best_loss.pth')\n",
    "            elif CFG.device == 'TPU':\n",
    "                if CFG.nprocs == 1:\n",
    "                    LOGGER.info(f'Epoch {epoch+1} - Save Best Loss: {best_loss:.4f} Model')\n",
    "                elif CFG.nprocs == 8:\n",
    "                    xm.master_print(f'Epoch {epoch+1} - Save Best Loss: {best_loss:.4f} Model')\n",
    "                xm.save({'model': model, \n",
    "                         'preds': preds}, \n",
    "                        OUTPUT_DIR+f'{CFG.model_name}_fold{fold}_best_loss.pth')\n",
    "                \n",
    "        if CFG.nprocs != 8:\n",
    "            check_point = torch.load(OUTPUT_DIR+f'{CFG.model_name}_fold{fold}_best_score.pth')\n",
    "            for c in [f'pred_{c}' for c in CFG.target_cols]:\n",
    "                valid_folds[c] = np.nan\n",
    "            valid_folds[[f'pred_{c}' for c in CFG.target_cols]] = check_point['preds']\n",
    "\n",
    "    return valid_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-06T07:51:23.986494Z",
     "iopub.status.busy": "2021-03-06T07:51:23.985701Z",
     "iopub.status.idle": "2021-03-06T07:51:23.989875Z",
     "shell.execute_reply": "2021-03-06T07:51:23.989396Z"
    },
    "papermill": {
     "duration": 0.041989,
     "end_time": "2021-03-06T07:51:23.990016",
     "exception": false,
     "start_time": "2021-03-06T07:51:23.948027",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# main\n",
    "# ====================================================\n",
    "def main():\n",
    "\n",
    "    \"\"\"\n",
    "    Prepare: 1.train  2.folds\n",
    "    \"\"\"\n",
    "\n",
    "    def get_result(result_df):\n",
    "        preds = result_df[[f'pred_{c}' for c in CFG.target_cols]].values\n",
    "        labels = result_df[CFG.target_cols].values\n",
    "        score, scores = get_score(labels, preds)\n",
    "        LOGGER.info(f'Score: {score:<.4f}  Scores: {np.round(scores, decimals=4)}')\n",
    "        \n",
    "    if CFG.train:\n",
    "        # train \n",
    "        oof_df = pd.DataFrame()\n",
    "        for fold in range(CFG.n_fold):\n",
    "            if fold in CFG.trn_fold:\n",
    "                _oof_df = train_loop(folds, fold)\n",
    "                oof_df = pd.concat([oof_df, _oof_df])\n",
    "                if CFG.nprocs != 8:\n",
    "                    LOGGER.info(f\"========== fold: {fold} result ==========\")\n",
    "                    get_result(_oof_df)\n",
    "                    \n",
    "        if CFG.nprocs != 8:\n",
    "            # CV result\n",
    "            LOGGER.info(f\"========== CV ==========\")\n",
    "            get_result(oof_df)\n",
    "            # save result\n",
    "            oof_df.to_csv(OUTPUT_DIR+'oof_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-06T07:51:24.052934Z",
     "iopub.status.busy": "2021-03-06T07:51:24.052063Z",
     "iopub.status.idle": "2021-03-06T10:32:58.367554Z",
     "shell.execute_reply": "2021-03-06T10:32:58.366521Z"
    },
    "papermill": {
     "duration": 9694.348882,
     "end_time": "2021-03-06T10:32:58.367704",
     "exception": false,
     "start_time": "2021-03-06T07:51:24.018822",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    if CFG.device == 'TPU':\n",
    "        def _mp_fn(rank, flags):\n",
    "            torch.set_default_tensor_type('torch.FloatTensor')\n",
    "            a = main()\n",
    "        FLAGS = {}\n",
    "        xmp.spawn(_mp_fn, args=(FLAGS,), nprocs=CFG.nprocs, start_method='fork')\n",
    "    elif CFG.device == 'GPU':\n",
    "        main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
