{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Summary\nHi, Kagglers. This is a public benchmark with resnet200d using `image_size = 512`. Since it's only the beginning of the competition, I feel like giving an idea of what can be achieved with solely images. Weights are available [here](http://https://www.kaggle.com/underwearfitting/resnet200d-baseline-benchmark-public).\n\nTraining details:\n- batch_size = 64\n- image_size = 512\n- lr = 3e-5\n- epochs = 30 with 1 epoch warmup at lr/10\n- augmentations: \n```\ntransforms_train = albumentations.Compose([\n     albumentations.RandomResizedCrop(image_size, image_size, scale=(0.9, 1), p=1), \n     albumentations.HorizontalFlip(p=0.5),\n     albumentations.ShiftScaleRotate(p=0.5),\n     albumentations.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=10, val_shift_limit=10, p=0.7),\n     albumentations.RandomBrightnessContrast(brightness_limit=(-0.2,0.2), contrast_limit=(-0.2, 0.2), p=0.7),\n     albumentations.CLAHE(clip_limit=(1,4), p=0.5),\n     albumentations.OneOf([\n         albumentations.OpticalDistortion(distort_limit=1.0),\n         albumentations.GridDistortion(num_steps=5, distort_limit=1.),\n         albumentations.ElasticTransform(alpha=3),\n     ], p=0.2),\n     albumentations.OneOf([\n         albumentations.GaussNoise(var_limit=[10, 50]),\n         albumentations.GaussianBlur(),\n         albumentations.MotionBlur(),\n         albumentations.MedianBlur(),\n     ], p=0.2),\n    albumentations.Resize(image_size, image_size),\n    albumentations.OneOf([\n    \tJpegCompression(),\n    \tDownscale(scale_min=0.1, scale_max=0.15),\n    ], p=0.2),\n    IAAPiecewiseAffine(p=0.2),\n    IAASharpen(p=0.2),\n    albumentations.Cutout(max_h_size=int(image_size * 0.1), max_w_size=int(image_size * 0.1), num_holes=5, p=0.5),\n    albumentations.Normalize(),\n])\n```\n- hardware: RTX3090 x 2"},{"metadata":{},"cell_type":"markdown","source":"## Configuration"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nbatch_size = 1\nimage_size = 512\ntta = True\n# submit = (len(pd.read_csv('../input/ranzcr-clip-catheter-line-classification/sample_submission.csv')) != 3582)\nsubmit = True\nenet_type = ['resnet200d'] * 5\nmodel_path = ['../input/resnet200d-baseline-benchmark-public/resnet200d_fold0_cv953.pth',\n              '../input/resnet200d-baseline-benchmark-public/resnet200d_fold1_cv955.pth',\n              '../input/resnet200d-baseline-benchmark-public/resnet200d_fold2_cv955.pth',\n              '../input/resnet200d-baseline-benchmark-public/resnet200d_fold3_cv957.pth',\n              '../input/resnet200d-baseline-benchmark-public/resnet200d_fold4_cv954.pth']\n# you can save GPU quota using fast sub attached in the last markdown file\nfast_sub = True\nfast_sub_path = '../input/xxxxxx/your_submission.csv'","execution_count":1,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Imports"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import os\nimport sys\n# sys.path.append('../input/pytorch-image-models/pytorch-image-models-master')\n# sys.path.append('../input/timm-pytorch-image-models/pytorch-image-models-master')\nsys.path.append('../input/pytorch-images-seresnet')\nimport numpy as np\nDEBUG = False\nimport time\nimport cv2\nimport PIL.Image\nfrom sklearn.metrics import accuracy_score\nfrom tqdm.notebook import tqdm\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.transforms as transforms\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nimport albumentations\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom pylab import rcParams\nimport timm\nfrom albumentations import *\nfrom albumentations.pytorch import ToTensorV2\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","execution_count":2,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"class RANZCRResNet200D(nn.Module):\n    def __init__(self, model_name='resnet200d', out_dim=11, pretrained=False):\n        super().__init__()\n        self.model = timm.create_model(model_name, pretrained=False)\n        n_features = self.model.fc.in_features\n        self.model.global_pool = nn.Identity()\n        self.model.fc = nn.Identity()\n        self.pooling = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Linear(n_features, out_dim)\n\n    def forward(self, x):\n        bs = x.size(0)\n        features = self.model(x)\n        pooled_features = self.pooling(features).view(bs, -1)\n        output = self.fc(pooled_features)\n        return output","execution_count":3,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Transforms"},{"metadata":{"trusted":true},"cell_type":"code","source":"transforms_test = albumentations.Compose([\n    Resize(image_size, image_size),\n    Normalize(\n         mean=[0.485, 0.456, 0.406],\n         std=[0.229, 0.224, 0.225],\n     ),\n    ToTensorV2()\n])","execution_count":4,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"class RANZCRDataset(Dataset):\n    def __init__(self, df, mode, transform=None):\n        \n        self.df = df.reset_index(drop=True)\n        self.mode = mode\n        self.transform = transform\n        self.labels = df[target_cols].values\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):\n        row = self.df.loc[index]\n        img = cv2.imread(row.file_path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        \n        if self.transform is not None:\n            res = self.transform(image=img)\n            img = res['image']\n        label = torch.tensor(self.labels[index]).float()\n        if self.mode == 'test':\n            return img\n        else:\n            return img, label","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv('../input/ranzcr-clip-catheter-line-classification/sample_submission.csv')\nif fast_sub:\n    test = pd.read_csv('../input/ranzcr-clip-catheter-line-classification/sample_submission.csv', nrows=5)\ntest['file_path'] = test.StudyInstanceUID.apply(lambda x: os.path.join('../input/ranzcr-clip-catheter-line-classification/test', f'{x}.jpg'))\ntarget_cols = test.iloc[:, 1:12].columns.tolist()\n\ntest_dataset = RANZCRDataset(test, 'test', transform=transforms_test)\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False,  num_workers=24)","execution_count":6,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Utils"},{"metadata":{"trusted":true},"cell_type":"code","source":"def inference_func(test_loader):\n    model.eval()\n    bar = tqdm(test_loader)\n    LOGITS = []\n    PREDS = []\n    \n    with torch.no_grad():\n        for batch_idx, images in enumerate(bar):\n            x = images.to(device)\n            logits = model(x)\n            LOGITS.append(logits.cpu())\n            PREDS += [logits.sigmoid().detach().cpu()]\n        PREDS = torch.cat(PREDS).cpu().numpy()\n        LOGITS = torch.cat(LOGITS).cpu().numpy()\n    return PREDS\n\ndef tta_inference_func(test_loader):\n    model.eval()\n    bar = tqdm(test_loader)\n    PREDS = []\n    LOGITS = []\n\n    with torch.no_grad():\n        for batch_idx, images in enumerate(bar):\n            x = images.to(device)\n            x = torch.stack([x,x.flip(-1), x.flip(-2)], 0) # hflip, vflip\n            x = x.view(-1, 3, image_size, image_size)\n            logits = model(x)\n            logits = logits.view(batch_size, 3, -1).mean(1)\n            PREDS += [logits.sigmoid().detach().cpu()]\n            LOGITS.append(logits.cpu())\n        PREDS = torch.cat(PREDS).cpu().numpy()\n        \n    return PREDS","execution_count":7,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"if submit:\n    test_preds = []\n    for i in range(len(enet_type)):\n        if enet_type[i] == 'resnet200d':\n            print('resnet200d loaded')\n            model = RANZCRResNet200D(enet_type[i], out_dim=len(target_cols))\n            model = model.to(device)\n        model.load_state_dict(torch.load(model_path[i], map_location=device))\n        if tta:\n            test_preds += [tta_inference_func(test_loader)]\n        else:\n            test_preds += [inference_func(test_loader)]\n\n    submission_ResNet200d_1 = pd.read_csv('../input/ranzcr-clip-catheter-line-classification/sample_submission.csv')\n    predictions_ResNet200d_1 = np.mean(test_preds, axis=0)\n#     submission_ResNet200D_1[target_cols] = np.mean(test_preds, axis=0)\n#     submission.to_csv('submission.csv', index=False)\nelse:\n    pd.read_csv('../input/ranzcr-clip-catheter-line-classification/sample_submission.csv').to_csv('submission.csv', index=False)","execution_count":9,"outputs":[{"output_type":"stream","text":"resnet200d loaded\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"29d1d4beba1540e7998fb754d3e7829b"}},"metadata":{}},{"output_type":"stream","text":"\nresnet200d loaded\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"858452c11d15491f9d83db20416ca82a"}},"metadata":{}},{"output_type":"stream","text":"\nresnet200d loaded\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e998753d4f8437a92523083bcb64550"}},"metadata":{}},{"output_type":"stream","text":"\nresnet200d loaded\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"912eae567d8a4d549df2d2b53389f004"}},"metadata":{}},{"output_type":"stream","text":"\nresnet200d loaded\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf44233d6d68463b96cf7cf2d176d5f7"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"##############################################################################","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport sys\nsys.path.append('../input/pytorch-images-seresnet')\n\nimport os\nimport math\nimport time\nimport random\nimport shutil\nfrom pathlib import Path\nfrom contextlib import contextmanager\nfrom collections import defaultdict, Counter\n\nimport scipy as sp\nimport numpy as np\nimport pandas as pd\n\nfrom tqdm.auto import tqdm\nfrom functools import partial\n\nimport cv2\nfrom PIL import Image\n\nfrom matplotlib import pyplot as plt\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim import Adam, SGD\nimport torchvision.models as models\nfrom torch.nn.parameter import Parameter\nfrom torch.utils.data import DataLoader, Dataset\nimport albumentations\nfrom albumentations import *\nfrom albumentations.pytorch import ToTensorV2\n\n\nimport timm\n\nfrom torch.cuda.amp import autocast, GradScaler\n\nimport warnings \nwarnings.filterwarnings('ignore')\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BATCH_SIZE = 32\nTEST_PATH = '../input/ranzcr-clip-catheter-line-classification/test'","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv('../input/ranzcr-clip-catheter-line-classification/sample_submission.csv')\nif fast_sub:\n    test = pd.read_csv('../input/ranzcr-clip-catheter-line-classification/sample_submission.csv', nrows=5)","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TestDataset(Dataset):\n    def __init__(self, df, transform=None):\n        self.df = df\n        self.file_names = df['StudyInstanceUID'].values\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        file_name = self.file_names[idx]\n        file_path = f'{TEST_PATH}/{file_name}.jpg'\n        image = cv2.imread(file_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        if self.transform:\n            augmented = self.transform(image=image)\n            image = augmented['image']\n        return image","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_transforms(image_size=640):\n        return Compose([\n            Resize(image_size, image_size),\n            Normalize(),\n            ToTensorV2(),\n        ])","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class PAM_Module(nn.Module):\n    \"\"\" Position attention module\"\"\"\n    #Ref from SAGAN\n    def __init__(self, in_dim):\n        super(PAM_Module, self).__init__()\n        self.chanel_in = in_dim\n\n        self.query_conv = nn.Conv2d(in_channels=in_dim, out_channels=in_dim//8, kernel_size=1)\n        self.key_conv = nn.Conv2d(in_channels=in_dim, out_channels=in_dim//8, kernel_size=1)\n        self.value_conv = nn.Conv2d(in_channels=in_dim, out_channels=in_dim, kernel_size=1)\n        self.gamma = nn.Parameter(torch.zeros(1))\n        \n    def forward(self, x):\n        \"\"\"\n            inputs :\n                x : input feature maps( B X C X H X W)\n            returns :\n                out : attention value + input feature\n                attention: B X (HxW) X (HxW)\n        \"\"\"\n        m_batchsize, C, height, width = x.size()\n        proj_query = self.query_conv(x).view(m_batchsize, -1, width*height).permute(0, 2, 1)\n        proj_key = self.key_conv(x).view(m_batchsize, -1, width*height)\n        energy = torch.bmm(proj_query, proj_key)\n        attention = torch.softmax(energy, dim=-1)\n        proj_value = self.value_conv(x).view(m_batchsize, -1, width*height)\n\n        out = torch.bmm(proj_value, attention.permute(0, 2, 1))\n        out = out.view(m_batchsize, C, height, width)\n\n        out = self.gamma*out + x\n        return out\n    \nclass CAM_Module(nn.Module):\n    \"\"\" Channel attention module\"\"\"\n    def __init__(self, in_dim):\n        super(CAM_Module, self).__init__()\n        self.chanel_in = in_dim\n        self.gamma = nn.Parameter(torch.zeros(1))\n        \n    def forward(self,x):\n        \"\"\"\n            inputs :\n                x : input feature maps( B X C X H X W)\n            returns :\n                out : attention value + input feature\n                attention: B X C X C\n        \"\"\"\n        m_batchsize, C, height, width = x.size()\n        proj_query = x.view(m_batchsize, C, -1)\n        proj_key = x.view(m_batchsize, C, -1).permute(0, 2, 1)\n        energy = torch.bmm(proj_query, proj_key)\n        energy_new = torch.max(energy, -1, keepdim=True)[0].expand_as(energy)-energy\n        attention = torch.softmax(energy_new, dim=-1)\n        proj_value = x.view(m_batchsize, C, -1)\n\n        out = torch.bmm(attention, proj_value)\n        out = out.view(m_batchsize, C, height, width)\n\n        out = self.gamma*out + x\n        return out\n    \nclass CBAM(nn.Module):\n    def __init__(self, in_channels):\n        # def __init__(self):\n        super(CBAM, self).__init__()\n        inter_channels = in_channels // 4\n        self.conv1_c = nn.Sequential(nn.Conv2d(in_channels, inter_channels, 3, padding=1, bias=False),\n                                     nn.BatchNorm2d(inter_channels),\n                                     nn.ReLU(inplace=True))\n        \n        self.conv1_s = nn.Sequential(nn.Conv2d(in_channels, inter_channels, 3, padding=1, bias=False),\n                                     nn.BatchNorm2d(inter_channels),\n                                     nn.ReLU(inplace=True))\n\n        self.channel_gate = CAM_Module(inter_channels)\n        self.spatial_gate = PAM_Module(inter_channels)\n        \n        self.conv2_c = nn.Sequential(nn.Conv2d(inter_channels, in_channels, 3, padding=1, bias=False),\n                                     nn.BatchNorm2d(in_channels),\n                                     nn.ReLU(inplace=True))\n        self.conv2_a = nn.Sequential(nn.Conv2d(inter_channels, in_channels, 3, padding=1, bias=False),\n                                     nn.BatchNorm2d(in_channels),\n                                     nn.ReLU(inplace=True))\n        \n    def forward(self, x):\n        feat1 = self.conv1_c(x)\n        chnl_att = self.channel_gate(feat1)\n        chnl_att = self.conv2_c(chnl_att)\n\n        feat2 = self.conv1_s(x)\n        spat_att = self.spatial_gate(feat2)\n        spat_att = self.conv2_a(spat_att)\n\n        x_out = chnl_att + spat_att\n\n        return x_out","execution_count":20,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class CustomResNet200D(nn.Module):\n    def __init__(self, model_name='resnet200d_320', pretrained=False):\n        super().__init__()\n        self.model = timm.create_model(model_name, pretrained=False)\n        if pretrained:\n            pretrained_path = '../input/resnet200d-pretrained-weight/resnet200d_ra2-bdba9bf9.pth'\n            self.model.load_state_dict(torch.load(pretrained_path))\n            print(f'load {model_name} pretrained model')\n        n_features = self.model.fc.in_features\n        self.model.global_pool = nn.Identity()\n        self.model.fc = nn.Identity()\n        self.pooling = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Linear(n_features, 11)\n\n        \n    def forward(self, x):\n        bs = x.size(0)\n        features = self.model(x)\n        pooled_features = self.pooling(features).view(bs, -1)\n        output = self.fc(pooled_features)\n        return output\n\nclass SeResNet152D(nn.Module):\n    def __init__(self, model_name='seresnet152d_320'):\n        super().__init__()\n        self.model = timm.create_model(model_name, pretrained=False)\n        n_features = self.model.fc.in_features\n        self.model.global_pool = nn.Identity()\n        self.model.fc = nn.Identity()\n        self.pooling = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Linear(n_features, 11)\n\n    def forward(self, x):\n        bs = x.size(0)\n        features = self.model(x)\n        pooled_features = self.pooling(features).view(bs, -1)\n        output = self.fc(pooled_features)\n        return output\n    \nclass EfficientNetB5(nn.Module):\n    def __init__(self, model_name='tf_efficientnet_b5_ns'):\n        super().__init__()\n        self.model = timm.create_model(model_name, pretrained=False)\n        n_features = self.model.classifier.in_features\n        self.model.global_pool = nn.Identity()\n        self.model.classifier = nn.Identity()\n        self.pooling = nn.AdaptiveAvgPool2d(1)\n        self.classifier = nn.Linear(n_features, 11)\n\n    def forward(self, x):\n        bs = x.size(0)\n        features = self.model(x)\n        pooled_features = self.pooling(features).view(bs, -1)\n        output = self.classifier(pooled_features)\n        return output\n    \nclass CustomResNet200D_WLF(nn.Module):\n    def __init__(self, model_name='resnet200d_320', pretrained=False):\n        super().__init__()\n        self.model = timm.create_model(model_name, pretrained=False)\n        n_features  = self.model.fc.in_features\n        self.model.fc = nn.Linear(n_features, 11)\n        \n        if pretrained:\n            pretrained_path = '../input/startingpointschestx/resnet200d_320_chestx.pth'\n#             self.model.load_state_dict(torch.load(pretrained_path, map_location=torch.device('cpu'))['model'])\n            \n            checkpoint = torch.load(pretrained_path, map_location='cpu')['model']\n            for key in list(checkpoint.keys()):\n                if 'model.' in key:\n                    checkpoint[key.replace('model.', '')] = checkpoint[key]\n                    del checkpoint[key]\n            self.model.load_state_dict(checkpoint) \n            \n            print(f'load {model_name} pretrained model')\n        n_features = self.model.fc.in_features\n        self.model.global_pool = nn.Identity()\n        self.model.fc = nn.Identity()\n        self.pooling = nn.AdaptiveAvgPool2d(1)\n        \n        self.local_fe = CBAM(n_features)\n        self.classifier = nn.Sequential(nn.Linear(n_features + n_features, n_features), \n                                        nn.BatchNorm1d(n_features),\n                                        nn.ReLU(inplace=True),\n                                        nn.Linear(n_features, 11))\n    def forward(self, x):\n        bs = x.size(0)\n        features = self.model(x)\n        \n        global_features = self.pooling(features).view(bs, -1)\n        \n        local_features = self.local_fe(features)\n        local_features = torch.sum(local_features, dim=[2, 3])  # ここ、どうしてsum？GAPではない？\n        \n        all_features = torch.cat([global_features, local_features], dim=1)\n        output = self.classifier(all_features)\n        \n        return output","execution_count":21,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def inference(models, test_loader, device):\n    tk0 = tqdm(enumerate(test_loader), total=len(test_loader))\n    probs = []\n    for i, (images) in tk0:\n        images = images.to(device)\n        avg_preds = []\n        for model in models:\n            with torch.no_grad():\n                y_preds1 = model(images)\n                y_preds2 = model(images.flip(-1))\n            y_preds = (y_preds1.sigmoid().to('cpu').numpy() + y_preds2.sigmoid().to('cpu').numpy()) / 2\n            avg_preds.append(y_preds)\n        avg_preds = np.mean(avg_preds, axis=0)\n        probs.append(avg_preds)\n    probs = np.concatenate(probs)\n    return probs","execution_count":22,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models200D_2 = []\nmodel = CustomResNet200D()\nmodel.load_state_dict(torch.load(\"../input/007-training-resnext-step3-data/resnet200d_320_fold0_best_loss_cpu.pth\")['model'])\nmodel.eval()\nmodel.to(device)\nmodels200D_2.append(model)\n\nmodels200D_3 = []\nmodel = CustomResNet200D_WLF()\nmodel.load_state_dict(torch.load(\"../input/007-training-resnet200d-step3-ver-42/resnet200d_320_fold0_best_loss.pth\")['model'])\nmodel.eval()\nmodel.to(device)\nmodels200D_3.append(model)\n\nmodels152D = []\nmodel = SeResNet152D()\nmodel.load_state_dict(torch.load('../input/training-seresnet152d-step3-2/seresnet152d_320_fold0_best_loss.pth', map_location='cpu')['model'])\nmodel.eval()\nmodel.to(device)\nmodels152D.append(model)\n\nmodelsEfficientNet = []\nmodel = EfficientNetB5()\nmodel.load_state_dict(torch.load('../input/efficientnetb5cv9621/tf_efficientnet_b5_ns_CV96.21.pth', map_location='cpu')['model'])\nmodel.eval()\nmodel.to(device)\nmodelsEfficientNet.append(model)","execution_count":24,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if submit:\n    test_dataset_640 = TestDataset(test, transform=get_transforms(image_size=640))\n    test_loader_640 = DataLoader(test_dataset_640, batch_size=BATCH_SIZE, shuffle=False, num_workers=4 , pin_memory=True)\n\n    predictions_ResNet200d_2 = inference(models200D_2, test_loader_640, device)\n    predictions_ResNet200d_3 = inference(models200D_3, test_loader_640, device)\n    predictions_SeResNet152d = inference(models152D, test_loader_640, device)\n    predictions_EfficientNet = inference(modelsEfficientNet, test_loader_640, device)\n    # predictions = (submission_ResNet200D_1 + predictions200d_2 + 0.50 * predictions152d) / 2.5","execution_count":25,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5cae2488c5d487083ad188f9d430f38"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f7420188ef643019067b5d2ca1dbe0b"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9b90a126d9a496da3ae00a6b2559939"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"11421282f58c4cf190e03b3b7b0456c5"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# target_cols = test.iloc[:, 1:12].columns.tolist()\n# test[target_cols] = predictions\n# test[['StudyInstanceUID'] + target_cols].to_csv('submission.csv', index=False)\n# test.head()","execution_count":26,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###########################################################################################################","execution_count":27,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import gc\nimport os\nimport sys\nimport time\nimport copy\nimport random\nimport shutil\nimport typing as tp\nfrom pathlib import Path\nfrom argparse import ArgumentParser\n\nimport yaml\nimport numpy as np\nimport pandas as pd\nfrom scipy.sparse import coo_matrix\nfrom sklearn.metrics import roc_auc_score\n\nfrom tqdm import tqdm\nfrom joblib import Parallel, delayed\n\nimport cv2\nimport albumentations\n\nfrom albumentations.core.transforms_interface import ImageOnlyTransform, DualTransform\nfrom albumentations.pytorch import ToTensorV2\n\nimport torch\nfrom torch import nn\nfrom torch.utils import data\nfrom torchvision import models as torchvision_models\n\nsys.path.append('../input/pytorch-image-models/pytorch-image-models-master')\nimport timm","execution_count":28,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ROOT = Path.cwd().parent\nINPUT = ROOT / \"input\"\nOUTPUT = ROOT / \"output\"\nDATA = INPUT / \"ranzcr-clip-catheter-line-classification\"\nTRAIN = DATA / \"train\"\nTEST = DATA / \"test\"\n\n\nTRAINED_MODEL = INPUT / \"ranzcr-clip-weights-for-multi-head-model-v2\"\nTMP = ROOT / \"tmp\"\nTMP.mkdir(exist_ok=True)\n\nRANDAM_SEED = 1086\nN_CLASSES = 11\nFOLDS = [0, 1, 2, 3, 4]\nN_FOLD = len(FOLDS)\nIMAGE_SIZE = (512, 512)\n\nCONVERT_TO_RANK = False\nFAST_COMMIT = True\n\nCLASSES = [\n    'ETT - Abnormal',\n    'ETT - Borderline',\n    'ETT - Normal',\n    'NGT - Abnormal',\n    'NGT - Borderline',\n    'NGT - Incompletely Imaged',\n    'NGT - Normal',\n    'CVC - Abnormal',\n    'CVC - Borderline',\n    'CVC - Normal',\n    'Swan Ganz Catheter Present'\n]","execution_count":29,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for p in DATA.iterdir():\n    print(p.name)\n\ntrain = pd.read_csv(DATA / \"train.csv\")\nsmpl_sub =  pd.read_csv(DATA / \"sample_submission.csv\")","execution_count":30,"outputs":[{"output_type":"stream","text":"train_tfrecords\nsample_submission.csv\ntrain_annotations.csv\ntest_tfrecords\ntrain.csv\ntest\ntrain\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"smpl_sub.shape","execution_count":31,"outputs":[{"output_type":"execute_result","execution_count":31,"data":{"text/plain":"(3582, 12)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"if fast_sub and len(smpl_sub) == 3582:\n    smpl_sub = pd.read_csv('../input/ranzcr-clip-catheter-line-classification/sample_submission.csv', nrows=5)","execution_count":32,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def multi_label_stratified_group_k_fold(label_arr: np.array, gid_arr: np.array, n_fold: int, seed: int=42):\n    \"\"\"\n    create multi-label stratified group kfold indexs.\n\n    reference: https://www.kaggle.com/jakubwasikowski/stratified-group-k-fold-cross-validation\n    input:\n        label_arr: numpy.ndarray, shape = (n_train, n_class)\n            multi-label for each sample's index using multi-hot vectors\n        gid_arr: numpy.array, shape = (n_train,)\n            group id for each sample's index\n        n_fold: int. number of fold.\n        seed: random seed.\n    output:\n        yield indexs array list for each fold's train and validation.\n    \"\"\"\n    np.random.seed(seed)\n    random.seed(seed)\n    start_time = time.time()\n    n_train, n_class = label_arr.shape\n    gid_unique = sorted(set(gid_arr))\n    n_group = len(gid_unique)\n\n    # # aid_arr: (n_train,), indicates alternative id for group id.\n    # # generally, group ids are not 0-index and continuous or not integer.\n    gid2aid = dict(zip(gid_unique, range(n_group)))\n#     aid2gid = dict(zip(range(n_group), gid_unique))\n    aid_arr = np.vectorize(lambda x: gid2aid[x])(gid_arr)\n\n    # # count labels by class\n    cnts_by_class = label_arr.sum(axis=0)  # (n_class, )\n\n    # # count labels by group id.\n    col, row = np.array(sorted(enumerate(aid_arr), key=lambda x: x[1])).T\n    cnts_by_group = coo_matrix(\n        (np.ones(len(label_arr)), (row, col))\n    ).dot(coo_matrix(label_arr)).toarray().astype(int)\n    del col\n    del row\n    cnts_by_fold = np.zeros((n_fold, n_class), int)\n\n    groups_by_fold = [[] for fid in range(n_fold)]\n    group_and_cnts = list(enumerate(cnts_by_group))  # pair of aid and cnt by group\n    np.random.shuffle(group_and_cnts)\n    print(\"finished preparation\", time.time() - start_time)\n    for aid, cnt_by_g in sorted(group_and_cnts, key=lambda x: -np.std(x[1])):\n        best_fold = None\n        min_eval = None\n        for fid in range(n_fold):\n            # # eval assignment.\n            cnts_by_fold[fid] += cnt_by_g\n            fold_eval = (cnts_by_fold / cnts_by_class).std(axis=0).mean()\n            cnts_by_fold[fid] -= cnt_by_g\n\n            if min_eval is None or fold_eval < min_eval:\n                min_eval = fold_eval\n                best_fold = fid\n\n        cnts_by_fold[best_fold] += cnt_by_g\n        groups_by_fold[best_fold].append(aid)\n    print(\"finished assignment.\", time.time() - start_time)\n\n    gc.collect()\n    idx_arr = np.arange(n_train)\n    for fid in range(n_fold):\n        val_groups = groups_by_fold[fid]\n\n        val_indexs_bool = np.isin(aid_arr, val_groups)\n        train_indexs = idx_arr[~val_indexs_bool]\n        val_indexs = idx_arr[val_indexs_bool]\n\n        print(\"[fold {}]\".format(fid), end=\" \")\n        print(\"n_group: (train, val) = ({}, {})\".format(n_group - len(val_groups), len(val_groups)), end=\" \")\n        print(\"n_sample: (train, val) = ({}, {})\".format(len(train_indexs), len(val_indexs)))\n\n        yield train_indexs, val_indexs","execution_count":33,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"label_arr = train[CLASSES].values\ngroup_id = train.PatientID.values\n\ntrain_val_indexs = list(\n    multi_label_stratified_group_k_fold(label_arr, group_id, N_FOLD, RANDAM_SEED))","execution_count":34,"outputs":[{"output_type":"stream","text":"finished preparation 0.12199878692626953\nfinished assignment. 0.9460818767547607\n[fold 0] n_group: (train, val) = (2591, 664) n_sample: (train, val) = (24062, 6021)\n[fold 1] n_group: (train, val) = (2600, 655) n_sample: (train, val) = (24124, 5959)\n[fold 2] n_group: (train, val) = (2613, 642) n_sample: (train, val) = (23966, 6117)\n[fold 3] n_group: (train, val) = (2608, 647) n_sample: (train, val) = (24143, 5940)\n[fold 4] n_group: (train, val) = (2608, 647) n_sample: (train, val) = (24037, 6046)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"fold\"] = -1\nfor fold_id, (trn_idx, val_idx) in enumerate(train_val_indexs):\n    train.loc[val_idx, \"fold\"] = fold_id\n    \ntrain.groupby(\"fold\")[CLASSES].sum()","execution_count":35,"outputs":[{"output_type":"execute_result","execution_count":35,"data":{"text/plain":"      ETT - Abnormal  ETT - Borderline  ETT - Normal  NGT - Abnormal  \\\nfold                                                                   \n0                 15               228          1448              56   \n1                 16               228          1448              56   \n2                 16               227          1448              56   \n3                 16               228          1448              55   \n4                 16               227          1448              56   \n\n      NGT - Borderline  NGT - Incompletely Imaged  NGT - Normal  \\\nfold                                                              \n0                  106                        550           960   \n1                  106                        550           960   \n2                  105                        549           959   \n3                  106                        550           959   \n4                  106                        549           959   \n\n      CVC - Abnormal  CVC - Borderline  CVC - Normal  \\\nfold                                                   \n0                639              1692          4265   \n1                639              1692          4265   \n2                639              1692          4265   \n3                639              1692          4265   \n4                639              1692          4264   \n\n      Swan Ganz Catheter Present  \nfold                              \n0                            166  \n1                            166  \n2                            166  \n3                            166  \n4                            166  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ETT - Abnormal</th>\n      <th>ETT - Borderline</th>\n      <th>ETT - Normal</th>\n      <th>NGT - Abnormal</th>\n      <th>NGT - Borderline</th>\n      <th>NGT - Incompletely Imaged</th>\n      <th>NGT - Normal</th>\n      <th>CVC - Abnormal</th>\n      <th>CVC - Borderline</th>\n      <th>CVC - Normal</th>\n      <th>Swan Ganz Catheter Present</th>\n    </tr>\n    <tr>\n      <th>fold</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>15</td>\n      <td>228</td>\n      <td>1448</td>\n      <td>56</td>\n      <td>106</td>\n      <td>550</td>\n      <td>960</td>\n      <td>639</td>\n      <td>1692</td>\n      <td>4265</td>\n      <td>166</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>16</td>\n      <td>228</td>\n      <td>1448</td>\n      <td>56</td>\n      <td>106</td>\n      <td>550</td>\n      <td>960</td>\n      <td>639</td>\n      <td>1692</td>\n      <td>4265</td>\n      <td>166</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>16</td>\n      <td>227</td>\n      <td>1448</td>\n      <td>56</td>\n      <td>105</td>\n      <td>549</td>\n      <td>959</td>\n      <td>639</td>\n      <td>1692</td>\n      <td>4265</td>\n      <td>166</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>16</td>\n      <td>228</td>\n      <td>1448</td>\n      <td>55</td>\n      <td>106</td>\n      <td>550</td>\n      <td>959</td>\n      <td>639</td>\n      <td>1692</td>\n      <td>4265</td>\n      <td>166</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>16</td>\n      <td>227</td>\n      <td>1448</td>\n      <td>56</td>\n      <td>106</td>\n      <td>549</td>\n      <td>959</td>\n      <td>639</td>\n      <td>1692</td>\n      <td>4264</td>\n      <td>166</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def resize_images(img_id, input_dir, output_dir, resize_to=(512, 512), ext=\"png\"):\n    img_path = input_dir / f\"{img_id}.jpg\"\n    save_path = output_dir / f\"{img_id}.{ext}\"\n    \n    img = cv2.imread(str(img_path), cv2.IMREAD_GRAYSCALE)\n    img = cv2.resize(img, resize_to)\n    cv2.imwrite(str(save_path), img, )\n\nTEST_RESIZED = TMP / \"test_{0}x{1}\".format(*IMAGE_SIZE)\nTEST_RESIZED.mkdir(exist_ok=True)\nTEST_RESIZED\n\n_ = Parallel(n_jobs=2, verbose=5)([\n    delayed(resize_images)(img_id, TEST, TEST_RESIZED, IMAGE_SIZE, \"png\")\n    for img_id in smpl_sub.StudyInstanceUID.values\n])","execution_count":36,"outputs":[{"output_type":"stream","text":"[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n[Parallel(n_jobs=2)]: Done   5 out of   5 | elapsed:    0.8s finished\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_activation(activ_name: str=\"relu\"):\n    \"\"\"\"\"\"\n    act_dict = {\n        \"relu\": nn.ReLU(inplace=True),\n        \"tanh\": nn.Tanh(),\n        \"sigmoid\": nn.Sigmoid(),\n        \"identity\": nn.Identity()}\n    if activ_name in act_dict:\n        return act_dict[activ_name]\n    else:\n        raise NotImplementedError\n        \n\nclass Conv2dBNActiv(nn.Module):\n    \"\"\"Conv2d -> (BN ->) -> Activation\"\"\"\n    \n    def __init__(\n        self, in_channels: int, out_channels: int,\n        kernel_size: int, stride: int=1, padding: int=0,\n        bias: bool=False, use_bn: bool=True, activ: str=\"relu\"\n    ):\n        \"\"\"\"\"\"\n        super(Conv2dBNActiv, self).__init__()\n        layers = []\n        layers.append(nn.Conv2d(\n            in_channels, out_channels,\n            kernel_size, stride, padding, bias=bias))\n        if use_bn:\n            layers.append(nn.BatchNorm2d(out_channels))\n            \n        layers.append(get_activation(activ))\n        self.layers = nn.Sequential(*layers)\n        \n    def forward(self, x):\n        \"\"\"Forward\"\"\"\n        return self.layers(x)\n        \n\nclass SSEBlock(nn.Module):\n    \"\"\"channel `S`queeze and `s`patial `E`xcitation Block.\"\"\"\n\n    def __init__(self, in_channels: int):\n        \"\"\"Initialize.\"\"\"\n        super(SSEBlock, self).__init__()\n        self.channel_squeeze = nn.Conv2d(\n            in_channels=in_channels, out_channels=1,\n            kernel_size=1, stride=1, padding=0, bias=False)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        \"\"\"Forward.\"\"\"\n        # # x: (bs, ch, h, w) => h: (bs, 1, h, w)\n        h = self.sigmoid(self.channel_squeeze(x))\n        # # x, h => return: (bs, ch, h, w)\n        return x * h\n    \n    \nclass SpatialAttentionBlock(nn.Module):\n    \"\"\"Spatial Attention for (C, H, W) feature maps\"\"\"\n    \n    def __init__(\n        self, in_channels: int,\n        out_channels_list: tp.List[int],\n    ):\n        \"\"\"Initialize\"\"\"\n        super(SpatialAttentionBlock, self).__init__()\n        self.n_layers = len(out_channels_list)\n        channels_list = [in_channels] + out_channels_list\n        assert self.n_layers > 0\n        assert channels_list[-1] == 1\n        \n        for i in range(self.n_layers - 1):\n            in_chs, out_chs = channels_list[i: i + 2]\n            layer = Conv2dBNActiv(in_chs, out_chs, 3, 1, 1, activ=\"relu\")\n            setattr(self, f\"conv{i + 1}\", layer)\n            \n        in_chs, out_chs = channels_list[-2:]\n        layer = Conv2dBNActiv(in_chs, out_chs, 3, 1, 1, activ=\"sigmoid\")\n        setattr(self, f\"conv{self.n_layers}\", layer)\n    \n    def forward(self, x):\n        \"\"\"Forward\"\"\"\n        h = x\n        for i in range(self.n_layers):\n            h = getattr(self, f\"conv{i + 1}\")(h)\n            \n        h = h * x\n        return h","execution_count":37,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class MultiHeadResNet200D(nn.Module):\n    \n    def __init__(\n        self, out_dims_head: tp.List[int]=[3, 4, 3, 1], pretrained=False\n    ):\n        \"\"\"\"\"\"\n        self.base_name = \"resnet200d_320\"\n        self.n_heads = len(out_dims_head)\n        super(MultiHeadResNet200D, self).__init__()\n        \n        # # load base model\n        base_model = timm.create_model(\n            self.base_name, num_classes=sum(out_dims_head), pretrained=False)\n        in_features = base_model.num_features\n        \n        if pretrained:\n            pretrained_model_path = '../input/startingpointschestx/resnet200d_320_chestx.pth'\n            state_dict = dict()\n            for k, v in torch.load(pretrained_model_path, map_location='cpu')[\"model\"].items():\n                if k[:6] == \"model.\":\n                    k = k.replace(\"model.\", \"\")\n                state_dict[k] = v\n            base_model.load_state_dict(state_dict)\n        \n        # # remove global pooling and head classifier\n        base_model.reset_classifier(0, '')\n        \n        # # Shared CNN Bacbone\n        self.backbone = base_model\n        \n        # # Multi Heads.\n        for i, out_dim in enumerate(out_dims_head):\n            layer_name = f\"head_{i}\"\n            layer = nn.Sequential(\n                SpatialAttentionBlock(in_features, [64, 32, 16, 1]),\n                nn.AdaptiveAvgPool2d(output_size=1),\n                nn.Flatten(start_dim=1),\n                nn.Linear(in_features, in_features),\n                nn.ReLU(inplace=True),\n                nn.Dropout(0.5),\n                nn.Linear(in_features, out_dim))\n            setattr(self, layer_name, layer)\n\n    def forward(self, x):\n        \"\"\"\"\"\"\n        h = self.backbone(x)\n        hs = [\n            getattr(self, f\"head_{i}\")(h) for i in range(self.n_heads)]\n        y = torch.cat(hs, axis=1)\n        return y\n    \n\n## forward test\nm = MultiHeadResNet200D([3, 4, 3, 1], False)\nm = m.eval()\n\nx = torch.rand(1, 3, 256, 256)\nwith torch.no_grad():\n    y = m(x)\nprint(\"[forward test]\")\nprint(\"input:\\t{}\\noutput:\\t{}\".format(x.shape, y.shape))\n\ndel m; del x; del y\ngc.collect()","execution_count":38,"outputs":[{"output_type":"stream","text":"[forward test]\ninput:\ttorch.Size([1, 3, 256, 256])\noutput:\ttorch.Size([1, 11])\n","name":"stdout"},{"output_type":"execute_result","execution_count":38,"data":{"text/plain":"0"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"class LabeledImageDataset(data.Dataset):\n    \"\"\"\n    Dataset class for (image, label) pairs\n\n    reads images and applys transforms to them.\n\n    Attributes\n    ----------\n    file_list : List[Tuple[tp.Union[str, Path], tp.Union[int, float, np.ndarray]]]\n        list of (image file, label) pair\n    transform_list : List[Dict]\n        list of dict representing image transform \n    \"\"\"\n\n    def __init__(\n        self,\n        file_list: tp.List[\n            tp.Tuple[tp.Union[str, Path], tp.Union[int, float, np.ndarray]]],\n        transform_list: tp.List[tp.Dict],\n    ):\n        \"\"\"Initialize\"\"\"\n        self.file_list = file_list\n        self.transform = ImageTransformForCls(transform_list)\n\n    def __len__(self):\n        \"\"\"Return Num of Images.\"\"\"\n        return len(self.file_list)\n\n    def __getitem__(self, index):\n        \"\"\"Return transformed image and mask for given index.\"\"\"\n        img_path, label = self.file_list[index]\n        img = self._read_image_as_array(img_path)\n        \n        img, label = self.transform((img, label))\n        return img, label\n\n    def _read_image_as_array(self, path: str):\n        \"\"\"Read image file and convert into numpy.ndarray\"\"\"\n        img_arr = cv2.imread(str(path))\n        img_arr = cv2.cvtColor(img_arr, cv2.COLOR_BGR2RGB)\n        return img_arr","execution_count":39,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_dataloaders_for_inference(\n    file_list: tp.List[tp.List], batch_size=32,\n):\n    \"\"\"Create DataLoader\"\"\"\n    dataset = LabeledImageDataset(\n        file_list,\n        transform_list=[\n          [\"Normalize\", {\n              \"always_apply\": True, \"max_pixel_value\": 255.0,\n              \"mean\": [\"0.4887381077884414\"], \"std\": [\"0.23064819430546407\"]}],\n          [\"ToTensorV2\", {\"always_apply\": True}],\n        ])\n    loader = data.DataLoader(\n        dataset,\n        batch_size=batch_size, shuffle=False,\n        num_workers=2, pin_memory=True,\n        drop_last=False)\n\n    return loader","execution_count":40,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class ImageTransformBase:\n    \"\"\"\n    Base Image Transform class.\n\n    Args:\n        data_augmentations: List of tuple(method: str, params :dict), each elems pass to albumentations\n    \"\"\"\n\n    def __init__(self, data_augmentations: tp.List[tp.Tuple[str, tp.Dict]]):\n        \"\"\"Initialize.\"\"\"\n        augmentations_list = [\n            self._get_augmentation(aug_name)(**params)\n            for aug_name, params in data_augmentations]\n        self.data_aug = albumentations.Compose(augmentations_list)\n\n    def __call__(self, pair: tp.Tuple[np.ndarray]) -> tp.Tuple[np.ndarray]:\n        \"\"\"You have to implement this by task\"\"\"\n        raise NotImplementedError\n\n    def _get_augmentation(self, aug_name: str) -> tp.Tuple[ImageOnlyTransform, DualTransform]:\n        \"\"\"Get augmentations from albumentations\"\"\"\n        if hasattr(albumentations, aug_name):\n            return getattr(albumentations, aug_name)\n        else:\n            return eval(aug_name)\n\n\nclass ImageTransformForCls(ImageTransformBase):\n    \"\"\"Data Augmentor for Classification Task.\"\"\"\n\n    def __init__(self, data_augmentations: tp.List[tp.Tuple[str, tp.Dict]]):\n        \"\"\"Initialize.\"\"\"\n        super(ImageTransformForCls, self).__init__(data_augmentations)\n\n    def __call__(self, in_arrs: tp.Tuple[np.ndarray]) -> tp.Tuple[np.ndarray]:\n        \"\"\"Apply Transform.\"\"\"\n        img, label = in_arrs\n        augmented = self.data_aug(image=img)\n        img = augmented[\"image\"]\n\n        return img, label","execution_count":41,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_setting_file(path: str):\n    \"\"\"Load YAML setting file.\"\"\"\n    with open(path) as f:\n        settings = yaml.safe_load(f)\n    return settings\n\n\ndef set_random_seed(seed: int = 42, deterministic: bool = False):\n    \"\"\"Set seeds\"\"\"\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)  # type: ignore\n    torch.backends.cudnn.deterministic = deterministic  # type: ignore\n    \n\ndef run_inference_loop(stgs, model, loader, device):\n    model.to(device)\n    model.eval()\n    pred_list = []\n    with torch.no_grad():\n        for x, t in tqdm(loader):\n            y = model(x.to(device))\n            pred_list.append(y.sigmoid().detach().cpu().numpy())\n            # pred_list.append(y.detach().cpu().numpy())\n        \n    pred_arr = np.concatenate(pred_list)\n    del pred_list\n    return pred_arr","execution_count":42,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if not torch.cuda.is_available():\n    device = torch.device(\"cpu\")\nelse:\n    device = torch.device(\"cuda\")\nprint(device)","execution_count":43,"outputs":[{"output_type":"stream","text":"cpu\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"if submit:\n    model_dir = TRAINED_MODEL\n    test_dir = TEST_RESIZED\n\n    test_file_list = [\n        (test_dir / f\"{img_id}.png\", [-1] * 11)\n        for img_id in smpl_sub[\"StudyInstanceUID\"].values]\n    test_loader = get_dataloaders_for_inference(test_file_list, batch_size=32)\n\n    test_preds_arr = np.zeros((N_FOLD, len(smpl_sub), N_CLASSES))    \n    for fold_id in FOLDS:\n        print(f\"[fold {fold_id}]\")\n        stgs = load_setting_file(model_dir / f\"fold{fold_id}\" / \"settings.yml\")\n        # # prepare \n        stgs[\"model\"][\"params\"][\"pretrained\"] = False\n        model = MultiHeadResNet200D(**stgs[\"model\"][\"params\"])\n        model_path = model_dir / f\"best_model_fold{fold_id}.pth\"\n        model.load_state_dict(torch.load(model_path, map_location=device))\n\n        # # inference test\n        test_pred = run_inference_loop(stgs, model, test_loader, device)\n        test_preds_arr[fold_id] = test_pred\n\n        del model\n        torch.cuda.empty_cache()\n        gc.collect()\n    test_preds_arr = np.mean(test_preds_arr, axis=0)","execution_count":44,"outputs":[{"output_type":"stream","text":"[fold 0]\n","name":"stdout"},{"output_type":"stream","text":"100%|██████████| 1/1 [00:13<00:00, 13.13s/it]\n","name":"stderr"},{"output_type":"stream","text":"[fold 1]\n","name":"stdout"},{"output_type":"stream","text":"100%|██████████| 1/1 [00:12<00:00, 12.34s/it]\n","name":"stderr"},{"output_type":"stream","text":"[fold 2]\n","name":"stdout"},{"output_type":"stream","text":"100%|██████████| 1/1 [00:12<00:00, 12.74s/it]\n","name":"stderr"},{"output_type":"stream","text":"[fold 3]\n","name":"stdout"},{"output_type":"stream","text":"100%|██████████| 1/1 [00:13<00:00, 13.15s/it]\n","name":"stderr"},{"output_type":"stream","text":"[fold 4]\n","name":"stdout"},{"output_type":"stream","text":"100%|██████████| 1/1 [00:12<00:00, 12.79s/it]\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"if submit:\n    predictions = (predictions_ResNet200d_1 + predictions_ResNet200d_2 + 0.50 * predictions_ResNet200d_3 + 0.50 * predictions_SeResNet152d + 0.10 * predictions_EfficientNet + test_preds_arr) / 4.1\n    sub = smpl_sub.copy()\n    sub[CLASSES] = predictions\n    sub.to_csv('submission.csv', index=False)\n    display(sub)","execution_count":46,"outputs":[{"output_type":"display_data","data":{"text/plain":"                                    StudyInstanceUID  ETT - Abnormal  \\\n0  1.2.826.0.1.3680043.8.498.46923145579096002617...        0.018368   \n1  1.2.826.0.1.3680043.8.498.84006870182611080091...        0.000048   \n2  1.2.826.0.1.3680043.8.498.12219033294413119947...        0.000064   \n3  1.2.826.0.1.3680043.8.498.84994474380235968109...        0.002121   \n4  1.2.826.0.1.3680043.8.498.35798987793805669662...        0.000147   \n\n   ETT - Borderline  ETT - Normal  NGT - Abnormal  NGT - Borderline  \\\n0          0.315060      0.591304        0.003793          0.007428   \n1          0.000164      0.000475        0.000216          0.000212   \n2          0.000120      0.000204        0.000233          0.000244   \n3          0.014899      0.024498        0.025777          0.008953   \n4          0.000328      0.000619        0.001126          0.000760   \n\n   NGT - Incompletely Imaged  NGT - Normal  CVC - Abnormal  CVC - Borderline  \\\n0                   0.027770      0.958853        0.040490          0.091036   \n1                   0.000303      0.000420        0.006782          0.010340   \n2                   0.000233      0.000299        0.008141          0.304478   \n3                   0.945091      0.026044        0.027175          0.049916   \n4                   0.000553      0.001395        0.012366          0.127769   \n\n   CVC - Normal  Swan Ganz Catheter Present  \n0      0.820525                    0.990823  \n1      0.981608                    0.000027  \n2      0.613475                    0.000113  \n3      0.844214                    0.001960  \n4      0.851373                    0.000100  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>StudyInstanceUID</th>\n      <th>ETT - Abnormal</th>\n      <th>ETT - Borderline</th>\n      <th>ETT - Normal</th>\n      <th>NGT - Abnormal</th>\n      <th>NGT - Borderline</th>\n      <th>NGT - Incompletely Imaged</th>\n      <th>NGT - Normal</th>\n      <th>CVC - Abnormal</th>\n      <th>CVC - Borderline</th>\n      <th>CVC - Normal</th>\n      <th>Swan Ganz Catheter Present</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.2.826.0.1.3680043.8.498.46923145579096002617...</td>\n      <td>0.018368</td>\n      <td>0.315060</td>\n      <td>0.591304</td>\n      <td>0.003793</td>\n      <td>0.007428</td>\n      <td>0.027770</td>\n      <td>0.958853</td>\n      <td>0.040490</td>\n      <td>0.091036</td>\n      <td>0.820525</td>\n      <td>0.990823</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.2.826.0.1.3680043.8.498.84006870182611080091...</td>\n      <td>0.000048</td>\n      <td>0.000164</td>\n      <td>0.000475</td>\n      <td>0.000216</td>\n      <td>0.000212</td>\n      <td>0.000303</td>\n      <td>0.000420</td>\n      <td>0.006782</td>\n      <td>0.010340</td>\n      <td>0.981608</td>\n      <td>0.000027</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.2.826.0.1.3680043.8.498.12219033294413119947...</td>\n      <td>0.000064</td>\n      <td>0.000120</td>\n      <td>0.000204</td>\n      <td>0.000233</td>\n      <td>0.000244</td>\n      <td>0.000233</td>\n      <td>0.000299</td>\n      <td>0.008141</td>\n      <td>0.304478</td>\n      <td>0.613475</td>\n      <td>0.000113</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.2.826.0.1.3680043.8.498.84994474380235968109...</td>\n      <td>0.002121</td>\n      <td>0.014899</td>\n      <td>0.024498</td>\n      <td>0.025777</td>\n      <td>0.008953</td>\n      <td>0.945091</td>\n      <td>0.026044</td>\n      <td>0.027175</td>\n      <td>0.049916</td>\n      <td>0.844214</td>\n      <td>0.001960</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1.2.826.0.1.3680043.8.498.35798987793805669662...</td>\n      <td>0.000147</td>\n      <td>0.000328</td>\n      <td>0.000619</td>\n      <td>0.001126</td>\n      <td>0.000760</td>\n      <td>0.000553</td>\n      <td>0.001395</td>\n      <td>0.012366</td>\n      <td>0.127769</td>\n      <td>0.851373</td>\n      <td>0.000100</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}