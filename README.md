# kaggle-RANZCR
<img width="947" alt="スクリーンショット 2021-02-23 18 48 36" src="https://user-images.githubusercontent.com/70050531/108826575-cbbde300-7607-11eb-9edd-35ac39b4e8cd.png">
Kaggleの RANZCR CLiP - Catheter and Line Position Challenge コンペのリポジトリです。<br>

## 方針
- Kaggle上で完結させる。<br>
- Version名は常にVersion〇〇に統一して、変更などはKaggle日記(このリポジトリのREADMD.md)に書き込む。<br> 

## Basics<br>
### Overview(Deepl)<br>
深刻な合併症は、患者のラインやチューブの位置を間違えた結果として発生する可能性があります。医師や看護師は、患者を管理する際にプロトコルに従っていることを確認するために、救命器具を配置するためのチェックリストを頻繁に使用しています。しかし、これらの手順には時間がかかることがあり、特に病院の収容人数が多いストレスの多い状況では、人為的なミスが発生しやすい。<br> 

入院患者は入院中にカテーテルやラインを挿入されることがありますが、位置を誤ると重篤な合併症を引き起こす可能性があります。経鼻胃管の気道内への位置ずれは、症例の最大3％で報告されており、これらの症例の最大40％が合併症を示しています[1-3]。手術室外で挿管された成人患者の気道チューブの位置異常は、最大25％の症例で報告されています[4,5]。合併症の可能性は、手技者の経験レベルと専門性の両方に直接関係している。位置がずれたチューブを早期に発見することは、危険な合併症（死に至ることもある）を防ぐ鍵であり、何百万人ものCOVID-19患者がこれらのチューブやラインを必要としている現在ではなおさらである。<br> 

ラインとチューブの位置を確認するためのゴールドスタンダードは胸部X線写真である。しかし、医師や放射線技師は、ラインやチューブが最適な位置にあるかどうかを確認するために、これらの胸部X線写真を手動でチェックしなければならない。これはヒューマンエラーの可能性を残すだけでなく、放射線技師は他のスキャンの報告に忙しくなるため、遅延が生じることもよくあります。ディープラーニングアルゴリズムは、カテーテルやラインの位置が間違っていることを自動的に検出することができるかもしれない。警告が出れば、臨床医は命に関わる合併症を避けるために、カテーテルの位置を変更したり、除去したりすることができる。<br> 

Royal Australian and New Zealand College of Radiologists（RANZCR）は、オーストラリア、ニュージーランド、シンガポールの臨床放射線技師と放射線腫瘍医のための非営利の専門組織である。RANZCRは、不適切な位置に置かれたチューブやラインを予防可能なものとして認識している世界の多くの医療機関（NHSを含む）の一つである。RANZCRは、そのようなエラーがキャッチされる安全システムの設計を支援しています。<br> 

このコンペティションでは、胸部レントゲン上のカテーテルやラインの存在と位置を検出します。機械学習を使用して、40,000枚の画像上でモデルをトレーニングしてテストし、不適切な位置にあるチューブを分類します。<br> 

データセットには、ラベル付けとの整合性を確保するために、一連の定義でラベル付けを行いました。正常カテゴリには、適切に配置され、再配置を必要としない線が含まれます。境界線のカテゴリには，理想的には多少の再配置を必要とするが，ほとんどの場合，現在の位置でも十分に機能するであろう線が含まれる．異常カテゴリーには、直ちに再配置を必要とするラインが含まれる。<br> 

成功すれば、臨床医の命を救うことができるかもしれない。COVID-19の症例が急増し続ける中、位置がずれたカテーテルやラインを早期に発見することはさらに重要である。多くの病院では定員に達しており、これらのチューブやラインを必要としている患者が増えています。カテーテルやラインの配置に関する迅速なフィードバックは、臨床医がこれらの患者の治療を改善するのに役立つ可能性がある。COVID-19以外にも、ラインやチューブの位置を検出することは、多くの病 院患者にとって常に必要なことである。<br> 

### data(DeepL)<br>
このコンテストでは、胸部レントゲン上のカテーテルやラインの存在と位置を検出します。機械学習を使用して、40,000枚の画像上でモデルを訓練してテストし、配置の悪いチューブを分類してください。<br> 

#### What files do I need?<br> 

トレーニング画像とテスト画像が必要です。このコンテストはコードのみのコンテストなので、隠れたテストセット（約4倍の大きさで14k枚の画像）もあります。<br> 
train.csvには画像ID、バイナリラベル、患者IDが含まれています。<br>
TFRecordsは訓練とテストの両方で利用可能です。(これらは隠れたテストセットでも利用可能です)。<br>
train_annotations.csvも含まれています。これらは、それを持つ訓練サンプルのセグメンテーションアノテーションです。これらは，競合他社のための追加情報としてのみ含まれています<br> 

#### Files<br>
- train.csv - 画像ID、バイナリラベル、患者IDが格納されています．
sample_submission.csv - 正しいフォーマットのサンプル投稿ファイルです．
- test - テスト画像
train - トレーニングイメージ

#### Columns<br>
- StudyInstanceUID - 各画像に固有のID
- ETT-異常-気管内チューブ留置異常
- ETT - 境界線 - 気管内チューブ留置境界線異常
- ETT - 正常 - 気管支内チューブの装着は正常です。
- NGT-異常-経鼻胃管留置異常
- NGT - 境界線 - 経鼻胃管留置境界線異常
- NGT - 画像化が不完全 - 画像化のために経鼻胃管留置が決定的ではない
- NGT - 正常 - 経鼻胃管留置は正常の境界線上にある。
- CVC - 異常 - 中心静脈カテーテル留置異常
- CVC - ボーダーライン - 中心静脈カテーテル留置境界異常
- CVC - 正常 - 中心静脈カテーテルの配置は正常です。
- スワンガンツカテーテルプレゼント
- PatientID - データセット内の各患者の一意のID

## Paper<br>

## Log<br>
### 20210223<br>
- join!!!<br>
- cassavaコンペのリベンジ。まだ3週間はあるので色々できるはず。<br>
- nb001(EDA)<br>
  - 自力でEDAを行った。
  - キャッサバコンペに比べて画像データがかなり大きい。512×512ぐらいに縮小してもいいとは思う。
  - 画像データは3チャネルあったが、3チャネル全てが同じであるため、平均をとって1チャネルの白黒画像にしても全く同じになる。それなら1チャネルだけの方が軽くていいと思う。<br>
  - 適当に閾値を設定して、それ以下の値のピクセルを全部0するのも良さそう。<br>
  - アノテーション画像を使うなら、画像データの縮小に合わせてアノテーション画像も縮小させる必要がある。(まだどう使えばいいのかよくわかってないけど...)<br>
  - クラスの割合が予想以上に不均衡だった。MoAコンペでは、ほとんどが0のクラスは、罰則を小さくすることしか学習できていなかったため、今回も気をつける必要があるかもしれない。<br>
  - とりあえず、trainingデータの平均でsample_submissionの表を埋めてsubmitしてみた。->LBは0.500だった。AUCだから当然か。<br>
- nb002(create_model_ResNext)<br>
  - ver1<br>
    - 再び、Y.NakamaさんのNotebookを写経。cassavaのときとほぼ一緒なのですぐ理解できた。汎用性が高いとはこういうことか。お陰様でいいスタートダッシュが切れた。感謝感謝。<br>
    - TrainDatasetまで書いた。いい復習になっている。<br>

### 20210224<br>
- [このNotebook](https://www.kaggle.com/raddar/errors-in-ett-abnormal-labels)を見ると、ETT -Abnormalのうちの一つがラベルが違うらしく、正確にはCVC - Abnormalらしい。EDAを見る限りでもそうっぽい。比較実験したい。<br>
- [このNotebook]の一番下のコメントで、timmとpytorch.xla_(だったっけ)のバージョンの齟齬の解消法が提案されてた。とりあえず、pytorch-image-modelsを使い続けて、xlaのバージョンを変えることにする。<br>
- どうやら、annotationを使うには、3stepでtrainingを行う必要があるらしい。<br>

- nb002<br>
  - ver2<br>
    - どうやら画像データのデータセットは自分で作らないといけないらしい。384×384で作ってから学習時に600×600にするのは、学習が早いかららしい。勉強のため、そうした場合と直接600×600に圧縮した場合の比較をしてみたい。恐らく、補完の際に若干崩れるため前者の精度は若干落ちると考えられる。<br>
    - 書き終わったので一旦quick saveした。<br>
  - ver3<br>
    - 公開Notebookとパラメータを同じにして動かした。<br>
    - ただし、時間短縮のため、foldは0のみにしてある。<br>
    - | CV | LB | train_loss | valid_loss |
      | :---: | :---: | :---: | :---: |
      | 0.9325 | 0.939 | 0.1152 | 0.1556 | <br>
    - CVはほぼ同じなので、恐らく再現できてる。
  - ver4<br>
    - ミスラベリングと議論されているデータを外した。また、全てのepochでモデルをセーブするようにした。<br>
      
- nb003(inference_ResNext)<br>
  - ver1<br>
    - 同じくY.NakamaさんのNotebookを写経した。<br>
    - 書き終わったので一旦quick saveした。<br>
  - ver2<br>
    - nb002_ver3の推論をしようとしたが、Internetを切り忘れた。<br>
  - ver3<br>
    - これ以降、trainingの方のNotebookに全て情報を書くため、このNotebookは登場させない。<br>
- nb004<br>
  - ver1<br>
    - 384×384の画像のデータセットを作るためのNotebook。nb002の元のNotebookにあったコメント通りにzipファイルにしてみた。<br>
    - 5GBもあるデータをいちいちローカルに落としてunzipして再びKaggleにあげ直すのは大変なので、Notebookの中でunzipして一つのファイルに入れるまでを行う。<br>
  - ver2<br>
    - unzipまで行った。<br>
    - データが取り出せない。なぜ。<br>
  - ver3<br>
    - 恐らく写真が多すぎるので諦めた。3stepの学習Notebookでは、画像サイズは普通に512だったため、そっちにする。<br>
- nb005(training_ResNeXt_step1)<br>
  - [Y,Nakamaさんのstep1]の写経。<br>
   - ver1<br>
     - TPUにとても苦戦する。よくわからんけど苦戦する。よくわからんから苦戦するのか。<br>
     - 一応全てのepochでモデルを保存するようにしておいた。<br>
     - 間違えてdebugをTrueにしたまま回してしまった。<br>
   - ver2<br>
     - debugをFalseに直した。<br>

### 20210225<br>
- nb002<br>
  - ver4<br>
    - ミスラベリングの疑いがあった、ETT - Abnormalクラスのデータを一つ外したところ、ETT - Abnormalクラスのaucが0.9336->0.9617と上がったので、恐らく指摘に間違いはない。その他のクラスのaucが若干下がってるのが気になるが、誤差としか考えられない。<br>
    - epoch5, 6を使うようにして、TTAを各5回にしたところ、LBが0.710まで落ちた。これは恐らく、学習の段階でaugmentationがRandomResizedCropとHorizontalFlipしかないことが原因だと思われる。<br>
    - TTAなどの変更は全て戻してsibmitした。<br>
    - | CV | LB | train_loss | valid_loss |
      | :---: | :---: | :---: | :---: |
      | 0.9308 | 0.941 | 0.1305 | 0.1551 | <br>
    - train_lossが高いのは、valid_lossが一番低いepochが手前にズレたから。<br>
    - やはり、ラベリングは間違っていたとみていいと思う。<br>
  - ver5<br>
    - Augmentationを増やしてみた。LB0.965を出している[このNotebook](https://www.kaggle.com/underwearfitting/single-fold-training-of-resnet200d-lb0-965)や、tawaraさんの[スレッド](https://www.kaggle.com/c/ranzcr-clip-catheter-line-classification/discussion/210016)でもAugmentationを増やすと精度が上がっているため、恐らく間違いない。<br>
    - 1epochは最終層以外固定した。本当はこのように複数の変更を同時に行うべきではないが、時間がないので仕方がない。前回のコンペで効いているので入れる。<br>
    - どちらがより大きい影響を及ぼしているかがわからないが、学習が遅くなってる。もうちょいepochを増やせばCVはまだ上がりそう。このnbの工夫はそのままnb006あたりに生かしたい。<br>
  - ver6<br>
    - lossがうまく下がらなかった。**たとえ時間がなくても一気に二つ以上変更を加えてはいけない。これからは守る。**<br>
  - ver8(ver7は失敗)<br>
    - augmentationを多めに入れた。<br>
    - | CV | LB | train_loss | valid_loss |
      | :---: | :---: | :---: | :---: |
      | 0.9332 | 0.936 | 0.1312 | 0.1530 | <br>
    - CVはよくなってるけどLBが悪くなっている。<br>
  - ver9<br>
    - ver4から、1epochは出力層以外を凍結するように変更した。<br>
    - | CV | LB | train_loss | valid_loss |
      | :---: | :---: | :---: | :---: |
      | 0.9292 | 0.936 | 0.1189 | 0.1563 | <br>
    - どちらも悪化してしまった。Cassavaコンペで効いたことがこっちでも一様に効くわけではないらしい。<br>
- nb006(ResNeXt_step2)<br>
  - ver3(ver1, ver2は失敗)<br>
    - 動かした。<br>
- nb007(ResNeXt_step3)<br>
  - ver5(ver1はquick save, ver2, ver3, ver4は失敗)<br>
    - とりあえず写して動かした。<br>
    - | CV | LB | train_loss | valid_loss |
      | :---: | :---: | :---: | :---: |
      | 0.9530 | 0.951 | 0.1107 | 0.0.1359 | <br>
- nb009(公開Notebook)<br>
  - ver1<br>
    - 公開されている状態から、TTAを8回に増やした。<br>
    - その後に、inferenceの関数内でモデルを5fold分ロードする形にしてみたところ、実行速度がかなり遅くなった。ボトルネックは恐らくモデルを毎回ロードしなければいけないところだと考えられる。loaderがボトルネックだと勝手に思い込んでいたが、実は違うらしい。工夫次第では他のNotebookもさらに早くなるかもしれない。<br>
    - LBは0.955だった。TTAを無闇に入れてもダメなのか。aucとaccuracyでは勝手が違うので難しい。<br>
  - ver3(ver2)は失敗<br>
    - 公開Notebook通りに戻した。LBは0.965。それはそうだ。<br>
  - ver4<br>
    - vertical flipだけ入れてみた。<br>
    - LBは0.965だが、表示桁以下は若干上がっている。<br>
  - ver6(ver5は失敗)<br>
    - verticalとhorizontalを同時にひっくり返すものも入れた。<br>
    - 0.963に下がった。よくわからんなあ...<br>
  - ver8(ver7は失敗)<br>
    - ver4から、RandomResizedCropを入れた。<br>
    - LBは0.964だった。<br>
  - ver9<br> 
    - ver8から、ShiftScaleRotateを入れた。<br>
    - LBは0.964だった。この取り組み、意味あるのかな...<br>
    
### 20210226<br>
- nb002<br>
  - ver9(ver8は失敗)<br>
    - ver4から、augmentationを追加した。nb005のaugmentationにverticalflipを追加したもの。<br>

### 20210227<br>
- 予測確率を1.1乗したりすれば、aucがよくなったりするのかと思っていたが、aucに関係するのは予測確率の順序だけであるため、全く意味がなかった。aucを大きく誤解していた。反省。ということは、最初の方にやった、trainデータの平均で各列を埋めたサブミットはaucが0.5になって当然となる。(閾値を変えても、FPR=TPR=0とFPR=TPR=1にしかならないため、左下と右上の角を結ぶことになるため当然面積は0.5になる。)<br>
- nb009<br>
  - ver8(ver7は失敗)<br>
    - ver4から、RandomResizedCropを入れた。<br>
    - LBは0.964だった。<br>
  - ver9<br> 
    - ver8から、ShiftScaleRotateを入れた。<br>
    - LBは0.964だった。この取り組み、意味あるのかな...<br>
- nb010(training_ViT)<br>
  - ver1<br>
    - Cassavaコンペ3位のモデルを参考に、672x672の画像を9分割して224x224のViTにそれぞれ突っ込んで、attentionでweight averagingをするモデルを作ってみた。<br>
    - 全然ダメだった。特にCVCクラスがひどすぎる。画像を切ってしまうと複数の画像にカテーテルが跨ってしまうのがよくないんだろうな...<br>
  - ver2<br>
    - 画像を448×448にして、４等分にしてみたけど同じくダメだった。この案はボツ。<br>

### 20210301<br>
- 鳥蛙コンペの反省会にお邪魔させていただいた。てっきり音声データは全く異質なことをしているのかを思いきや割と画像コンペの側面が強かったよう。色々勉強させていただいた。
- nb011<br>
  - nb002_ver4をEfficientNetB3nsでやってみる。<br>
  - 伸びが明らかによくなかった。DiscussionでもEfficientNetを使っている人はほとんどいないようなので、大人しくResNet200Dを使おうと思う。<br>

### 20210302<br>
- Discussionを色々みた結果、一番強そうなのはやはりResNet200Dだった。また、4stage trainingが強そうだった。<br>
- annotationをどう使うかが今回の鍵になっていそう。一つの案としては、4stageにおける、3stageと4stageを交互に繰り返していくことがあげられる。そうすることで、よりteacherモデルの重みに近づけることができると予想。ちなみに、着想は鳥蛙コンペ5位のチームがやっていたcycle pseudo labelingから得た。<br>
- **ひらさんとマージした！！！ yukies爆誕！！！ 頑張るぞ〜**
- nb005<br>
  - ver5<br>
    - 4stageのモデルは画像サイズを640にしていたため、teacherモデルも画像サイズを640にした。<br>
- nb006<br>
  - ver4<br>
    - nb005_ver5の2stage。画像サイズを640x640にしたため、バッチサイズを16から8に落とした。本来は学習率も一緒に落とすが、そのままでもいけそうなのでそのままにした。epochを10に伸ばした。<br>
    - 少し動かした感じだと、学習率はやはり小さくするべきだった。あと、epochは10だと長すぎる。<br>
  - ver5<br>
    - 学習率を落として、epochも戻した。ほぼ公開Notebookのまま。やはり最初はそうしないと比較ができない。対照実験を守るべし。<br>
    - | CV | train_loss | valid_loss |
      | :---: | :---: | :---: | 
      | 0.9380 | 2.0690 | 0.1601 | <br>
    - valid_lossが一番低いのがepoch2なのが気になる。学習率を落とした方がいいっぽい。<br>

### 20210303<br>
- annotationが間違ってる画像があるという意見がある。[このディスカッション](https://www.kaggle.com/c/ranzcr-clip-catheter-line-classification/discussion/210064)に書かれていた。外した方がスコアが高い可能性もなくはない。<br>
- CVCのスコアはこのコンペの一つに鍵になっていると思う。ただ、[このディスカッション](https://www.kaggle.com/c/ranzcr-clip-catheter-line-classification/discussion/216219)によると、CVCのAUCスコアが低いのはモデルが苦戦しているからではなく、CVCがほぼ全てのデータに入っているからだそう。[このディスカッション](https://www.kaggle.com/c/ranzcr-clip-catheter-line-classification/discussion/207602)に外部データがある。使えるかも。<br>
- nb006<br>
  - ver6 ~ ver8<br>
    - 学習率を変える実験をしたので色々書く(ver5から載せる)<br>
    - | learning rate (ver) |  CV | train_loss | valid_loss |
      | :---: | :---: | :---: | :---: | 
      | 2.5e-4(ver5) | 0.9380 | 2.0690 | 0.1601 | 
      | 1e-4(ver6) | 0.9454 | 1.7504 | 0.1573 | 
      | 5e-5(ver7) | 0.9486 | 1.7765 | 0.1492 | 
      | 3e-5(ver8) | 0.9507 | 1.9575 | 0.1360 |
      | 2e-5(ver10) | 0.9513 | 1.9946 | 0.1353 | 
      | 1e-5(ver9) | 0.9525 | 2.2128 | 0.1306 | 
    - 公開KernelのCVが0.9234であることを考えるとかなりよくなっている(一旦全てのstageを通過させたモデルなので単純比較はできないけど)。<br>
    - 学習率を落とすとepoch1のCVが高くなるが、それはもともとの重みが反映されているだけ。ここでの目的は親モデルの重みに近づけることであるため、train_lossもしっかり下がっているやつを選びたい。<br>
    - 2e-5, 1e-5に関しては上記の理由でepoch5のモデルを取っている。<br>
    - 以上から、ver10のモデルを採用する。<br>
- nb007<br> 
  - ver7(ver6は失敗)<br>
    - nb006_ver10_epoch5のモデルを使う。ハイパラは全て同じ。<br>
